{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextualEntailment/persiannlp/mt5-base-parsinlu-snli-entailment-GPU.ipynb","provenance":[{"file_id":"1EQ3ynbjhKCXBNzYhWi7_2feFY0Tw1UE7","timestamp":1625385106160},{"file_id":"1c5WvqQKKZnab7GsUVa6fi1Q8vYwvlLTc","timestamp":1624943579756}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d604494f9acf4342bf26f42b78c2570a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_898f5f6d430e40b2bebf610a7c489850","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e688c77d8a8c441292ef18381d15efc7","IPY_MODEL_28fdb2a436844380b33f954a8c0a6440"]}},"898f5f6d430e40b2bebf610a7c489850":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e688c77d8a8c441292ef18381d15efc7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5f034ccd936f4d62a421cadf46211bb3","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":4309802,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4309802,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_79a870f79247473293615daf1d4f46f7"}},"28fdb2a436844380b33f954a8c0a6440":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0ad68ef6af2f41d3a4b57115f17d7948","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 4.31M/4.31M [00:04&lt;00:00, 1.07MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b49e23ac620f460ea93bf0dc1d7705d7"}},"5f034ccd936f4d62a421cadf46211bb3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"79a870f79247473293615daf1d4f46f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0ad68ef6af2f41d3a4b57115f17d7948":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b49e23ac620f460ea93bf0dc1d7705d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"90aa5816ed224998bb05c2114da4f3c2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_929bf109b6784deda1bfcd9e3aac87e3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_82bf0ffa71c2478e9652246adea41778","IPY_MODEL_9fd2b4af1a86406986a2e012ce09a47e"]}},"929bf109b6784deda1bfcd9e3aac87e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"82bf0ffa71c2478e9652246adea41778":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_04b9fe96203548b68152e59b02ba3e55","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":65,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":65,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_76df09b4a901419aa5c2e24663b8ff54"}},"9fd2b4af1a86406986a2e012ce09a47e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1340cd2705694e498ef875e65eca3d04","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 65.0/65.0 [03:35&lt;00:00, 3.32s/B]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0bdbf43090524f2da0918dddb2bdf0c2"}},"04b9fe96203548b68152e59b02ba3e55":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"76df09b4a901419aa5c2e24663b8ff54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1340cd2705694e498ef875e65eca3d04":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0bdbf43090524f2da0918dddb2bdf0c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"587aa494792249c6a7edc31f4c9b184d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_74756b5f9ea94cfca668ce854f6b8e92","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e8d09795a7d8435da93e0060245d3085","IPY_MODEL_a174035594474b4aaa1429f9a5dfc962"]}},"74756b5f9ea94cfca668ce854f6b8e92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8d09795a7d8435da93e0060245d3085":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_158eeeadce7348f8950fc667e5189d53","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":375,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":375,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8672001d2f434214a3f4299b912bb148"}},"a174035594474b4aaa1429f9a5dfc962":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c97d297db279432f84935339c321410e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 375/375 [00:03&lt;00:00, 123B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_47801d0e71a1435385c3169726d8821b"}},"158eeeadce7348f8950fc667e5189d53":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8672001d2f434214a3f4299b912bb148":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c97d297db279432f84935339c321410e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"47801d0e71a1435385c3169726d8821b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f5b2b98a20fc408ba3d6d9aab03d1178":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c304b1f832084cb3a38b256a2a1f30e5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e8b6cdae65b04c7298194c679154bca6","IPY_MODEL_6e91e1faf64e4c7091176886a578cc23"]}},"c304b1f832084cb3a38b256a2a1f30e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8b6cdae65b04c7298194c679154bca6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d9fdca1317cd49e5906a542a2163dfb6","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":695,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":695,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f0f5de7e6a3643c5bfec7203943e92a3"}},"6e91e1faf64e4c7091176886a578cc23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e3a59ef75d0b4f1ab41a24d5208321bd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 695/695 [00:00&lt;00:00, 1.84kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6a5bef87d8bb46f48f15c18eeafcce17"}},"d9fdca1317cd49e5906a542a2163dfb6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f0f5de7e6a3643c5bfec7203943e92a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e3a59ef75d0b4f1ab41a24d5208321bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6a5bef87d8bb46f48f15c18eeafcce17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d925eb994be34f8891e142ab1f62a588":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5b5a6fd95e1e4ecb9d35e0ce2f982708","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5ed034cbdd8f4ce987d5995774036206","IPY_MODEL_c58af0b899ae4a8287cb0737e6b21d5f"]}},"5b5a6fd95e1e4ecb9d35e0ce2f982708":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5ed034cbdd8f4ce987d5995774036206":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7b644720910e4eeebf4b50d27d5e8bcd","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":2329703923,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2329703923,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_867f9634a1ca4c62b31a1a0b625b9965"}},"c58af0b899ae4a8287cb0737e6b21d5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f31a9636b5984415bd8a16034ead29e4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.33G/2.33G [03:28&lt;00:00, 11.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7ac4a7405b2d408a9f1e0134793a178c"}},"7b644720910e4eeebf4b50d27d5e8bcd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"867f9634a1ca4c62b31a1a0b625b9965":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f31a9636b5984415bd8a16034ead29e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7ac4a7405b2d408a9f1e0134793a178c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"EVTLnae70XUx"},"source":["# Textual Entailment"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MYkRN9VN7dHk","executionInfo":{"status":"ok","timestamp":1628064661627,"user_tz":-270,"elapsed":9,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"02aa0505-f67d-476b-b42a-84f61573d698"},"source":["!nvidia-smi\n","!lscpu"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Wed Aug  4 08:11:03 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Architecture:        x86_64\n","CPU op-mode(s):      32-bit, 64-bit\n","Byte Order:          Little Endian\n","CPU(s):              2\n","On-line CPU(s) list: 0,1\n","Thread(s) per core:  2\n","Core(s) per socket:  1\n","Socket(s):           1\n","NUMA node(s):        1\n","Vendor ID:           GenuineIntel\n","CPU family:          6\n","Model:               79\n","Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n","Stepping:            0\n","CPU MHz:             2199.998\n","BogoMIPS:            4399.99\n","Hypervisor vendor:   KVM\n","Virtualization type: full\n","L1d cache:           32K\n","L1i cache:           32K\n","L2 cache:            256K\n","L3 cache:            56320K\n","NUMA node0 CPU(s):   0,1\n","Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rzVbON7h7dKS","executionInfo":{"status":"ok","timestamp":1628064701757,"user_tz":-270,"elapsed":40133,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"da879862-fefd-4f3e-e89a-edcee34cf47d"},"source":["!pip install hazm==0.7.0\n","!pip install seqeval==1.2.2\n","!pip install sentencepiece==0.1.96\n","!pip install transformers==4.7.0\n","!pip install clean-text[gpl]==0.4.0\n","!pip install sacrebleu==1.5.1"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting hazm==0.7.0\n","  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n","\u001b[K     |████████████████████████████████| 316 kB 4.0 MB/s \n","\u001b[?25hCollecting nltk==3.3\n","  Downloading nltk-3.3.0.zip (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 6.2 MB/s \n","\u001b[?25hCollecting libwapiti>=0.2.1\n","  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n","\u001b[K     |████████████████████████████████| 233 kB 19.3 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm==0.7.0) (1.15.0)\n","Building wheels for collected packages: nltk, libwapiti\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394488 sha256=144737514298933a4eaffe6ca8f5ae540d1872dac8c6ea1b54684cefc7581a7f\n","  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n","  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154227 sha256=e2bc07368a2af2ca5c8ac126ba3f062e647dcc1947231e672bde7d52cd0d0f61\n","  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n","Successfully built nltk libwapiti\n","Installing collected packages: nltk, libwapiti, hazm\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n","Collecting seqeval==1.2.2\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 931 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval==1.2.2) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval==1.2.2) (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=60f052602c325d17ef3a5e1baf9a4bced6223d00f8ea86aa871ed19ff0a47b40\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n","Collecting sentencepiece==0.1.96\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 4.2 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Collecting transformers==4.7.0\n","  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (3.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 36.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (4.41.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (4.6.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (21.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 39.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (1.19.5)\n","Collecting huggingface-hub==0.0.8\n","  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.7.0) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.7.0) (3.5.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0) (1.15.0)\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.7.0\n","Collecting clean-text[gpl]==0.4.0\n","  Downloading clean_text-0.4.0-py3-none-any.whl (9.8 kB)\n","Collecting emoji\n","  Downloading emoji-1.4.2.tar.gz (184 kB)\n","\u001b[K     |████████████████████████████████| 184 kB 5.9 MB/s \n","\u001b[?25hCollecting ftfy<7.0,>=6.0\n","  Downloading ftfy-6.0.3.tar.gz (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 2.4 MB/s \n","\u001b[?25hCollecting unidecode<2.0.0,>=1.1.1\n","  Downloading Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)\n","\u001b[K     |████████████████████████████████| 241 kB 38.2 MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text[gpl]==0.4.0) (0.2.5)\n","Building wheels for collected packages: ftfy, emoji\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41934 sha256=8c114efe0afb91eb49987153cb02e9f93bbc04795ff92b1094ba674f21e26e51\n","  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186469 sha256=c0f28d169cc2a1e836656b204d7894e25d46a099e1ed9eb4ceb5ba9c765d93c1\n","  Stored in directory: /root/.cache/pip/wheels/e4/61/e7/2fc1ac8f306848fc66c6c013ab511f0a39ef4b1825b11363b2\n","Successfully built ftfy emoji\n","Installing collected packages: ftfy, emoji, unidecode, clean-text\n","Successfully installed clean-text-0.4.0 emoji-1.4.2 ftfy-6.0.3 unidecode-1.2.0\n","Collecting sacrebleu==1.5.1\n","  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 1.7 MB/s \n","\u001b[?25hCollecting portalocker==2.0.0\n","  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n","Installing collected packages: portalocker, sacrebleu\n","Successfully installed portalocker-2.0.0 sacrebleu-1.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o_zHi1zlwyXb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628064767865,"user_tz":-270,"elapsed":66120,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"87ff58a3-d175-4853-96b5-4d40386b3647"},"source":["!pip install PyDrive\n","import os\n","import IPython.display as ipd\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: PyDrive in /usr/local/lib/python3.7/dist-packages (1.3.1)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (4.1.3)\n","Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (1.12.8)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (3.13)\n","Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\n","Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n","Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.32.1)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n","Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.26.3)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.23.0)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (21.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2018.9)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (57.2.0)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.17.3)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.53.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (0.2.8)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.24.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4hU6ggJvK_FO","executionInfo":{"status":"ok","timestamp":1628064780839,"user_tz":-270,"elapsed":7823,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"1b8571a3-6876-4382-9432-2bda54fe6c9a"},"source":["# Import required packages\n","import os\n","import gc\n","import re\n","import hazm\n","import time\n","import json\n","import sacrebleu\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import transformers\n","from transformers import AutoConfig, AutoTokenizer\n","from transformers import AutoModelForSequenceClassification\n","from transformers import MT5Config, MT5ForConditionalGeneration, MT5Tokenizer\n","from transformers.data.processors.squad import SquadV2Processor\n","from transformers.data.metrics.squad_metrics import normalize_answer, compute_exact, compute_f1, merge_eval, \\\n","    make_eval_dict, apply_no_ans_threshold, find_all_best_thresh, squad_evaluate\n","\n","from cleantext import clean\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","print()\n","print('numpy', np.__version__)\n","print('pandas', pd.__version__)\n","print('transformers', transformers.__version__)\n","print('torch', torch.__version__)\n","print()\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n","numpy 1.19.5\n","pandas 1.1.5\n","transformers 4.7.0\n","torch 1.9.0+cu102\n","\n","There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZxED0_YSK_H-","executionInfo":{"status":"ok","timestamp":1628064784762,"user_tz":-270,"elapsed":3928,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}}},"source":["class TextualEntailmentDataset(torch.utils.data.Dataset):\n","    \"\"\" Create a PyTorch dataset for Textual Entailment. \"\"\"\n","\n","    def __init__(self, premises_hypotheses, targets, label_list, tokenizer, model_type, max_length):\n","        self.premises_hypotheses = premises_hypotheses\n","        self.tokenizer = tokenizer\n","        self.model_type = model_type\n","        self.targets = targets\n","        self.max_length = max_length\n","        self.label2index = {label: i for i, label in enumerate(label_list)} if isinstance(label_list, list) else {}\n","        self.index2label = {i: label for label, i in self.label2index.items()}\n","\n","    def __len__(self):\n","        return len(self.premises_hypotheses)\n","\n","    def __getitem__(self, item):\n","        encoding = self.tokenizer(\n","            self.premises_hypotheses[item],\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            truncation=True,\n","            padding='max_length',\n","            return_tensors=\"pt\"\n","        )\n","        if self.model_type == \"mt5\":\n","            inputs = {\n","                'premise': self.premises_hypotheses[item].split('<sep>')[0],\n","                'hypothesis': self.premises_hypotheses[item].split('<sep>')[1],\n","                'pair': self.premises_hypotheses[item],\n","                'targets': torch.tensor(self.label2index[self.targets[item]], dtype=torch.long),\n","                'original_targets': self.targets[item],\n","                'input_ids': encoding.input_ids.flatten(),\n","                'attention_mask': encoding.attention_mask.flatten()\n","            }\n","        else:\n","            inputs = {\n","                'premise': self.premises_hypotheses[item].split('<sep>')[0],\n","                'hypothesis': self.premises_hypotheses[item].split('<sep>')[1],\n","                'pair': self.premises_hypotheses[item],\n","                'targets': torch.tensor(self.label2index[self.targets[item]], dtype=torch.long),\n","                'original_targets': self.targets[item],\n","                'input_ids': encoding.input_ids.flatten(),\n","                'attention_mask': encoding.attention_mask.flatten(),\n","                'token_type_ids': encoding['token_type_ids'].flatten()\n","            }\n","        return inputs\n","\n","\n","class TextualEntailmentDataset2(torch.utils.data.Dataset):\n","    \"\"\" Create a PyTorch dataset for Textual Entailment. \"\"\"\n","\n","    def __init__(self, premises, hypotheses, targets, label_list, tokenizer, model_type, max_length):\n","        self.premises = premises\n","        self.hypotheses = hypotheses\n","        self.tokenizer = tokenizer\n","        self.model_type = model_type\n","        self.targets = targets\n","        self.max_length = max_length\n","        self.label2index = {label: i for i, label in enumerate(label_list)} if isinstance(label_list, list) else {}\n","        self.index2label = {i: label for label, i in self.label2index.items()}\n","\n","    def __len__(self):\n","        return len(self.premises)\n","\n","    def __getitem__(self, item):\n","        encoding = self.tokenizer(\n","            [(self.premises[item], self.hypotheses[item])],\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            truncation=True,\n","            padding='max_length',\n","            return_tensors=\"pt\"\n","        )\n","        if self.model_type == \"mt5\":\n","            inputs = {\n","                'premise': self.premises[item],\n","                'hypothesis': self.hypotheses[item],\n","                'targets': torch.tensor(self.label2index[self.targets[item]], dtype=torch.long),\n","                'original_targets': self.targets[item],\n","                'input_ids': encoding.input_ids.flatten(),\n","                'attention_mask': encoding.attention_mask.flatten()\n","            }\n","        else:\n","            inputs = {\n","                'premise': self.premises[item],\n","                'hypothesis': self.hypotheses[item],\n","                'targets': torch.tensor(self.label2index[self.targets[item]], dtype=torch.long),\n","                'original_targets': self.targets[item],\n","                'input_ids': encoding.input_ids.flatten(),\n","                'attention_mask': encoding.attention_mask.flatten(),\n","                'token_type_ids': encoding['token_type_ids'].flatten()\n","            }\n","        return inputs\n","\n","\n","class TextualEntailmentSimilarityDataset(torch.utils.data.Dataset):\n","    \"\"\" Create a PyTorch dataset for Textual Entailment. \"\"\"\n","\n","    def __init__(self, premises, hypotheses, targets):\n","        self.premises = premises\n","        self.hypotheses = hypotheses\n","        self.targets = targets\n","\n","    def __len__(self):\n","        return len(self.premises)\n","\n","    def __getitem__(self, item):\n","        inputs = {\n","            'item': item,\n","            'premise': self.premises[item],\n","            'hypothesis': self.hypotheses[item],\n","            'targets': self.targets[item]\n","        }\n","        return inputs\n","\n","\n","class TextualEntailment:\n","    def __init__(self, model_name, model_type, label_list):\n","        self.normalizer = hazm.Normalizer()\n","        self.model_name = model_name\n","        self.model_type = model_type.lower()\n","        self.label_list = label_list\n","        if self.model_type == \"mt5\":\n","            self.tokenizer = MT5Tokenizer.from_pretrained(model_name)\n","            self.model = MT5ForConditionalGeneration.from_pretrained(model_name)\n","            self.config = MT5Config.from_pretrained(self.model_name)\n","        elif self.model_type == \"sentence-transformer\":\n","            word_embedding_model = models.Transformer(model_name)\n","            pooling_model = models.Pooling(\n","                word_embedding_model.get_word_embedding_dimension(),\n","                pooling_mode_mean_tokens=True,\n","                pooling_mode_cls_token=False,\n","                pooling_mode_max_tokens=False\n","            )\n","            self.model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","            self.config = AutoConfig.from_pretrained(self.model_name)\n","        else:\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n","            self.config = AutoConfig.from_pretrained(self.model_name)\n","            self.id2label = self.config.id2label\n","            self.label2id = self.config.label2id\n","\n","    def load_dataset_test_file(self, dataset_name, dataset_file, **kwargs):\n","        if dataset_name.lower() in [\"parsinlu-natural\", \"parsinlu-mnli\", \"parsinlu-farstail\"]:\n","            if not os.path.exists(dataset_file):\n","                print(f'{dataset_file} not exists!')\n","                return\n","            data = pd.read_csv(dataset_file, delimiter=\"\\t\", names=['premise_hypothesis', 'label'], header=None)\n","\n","            # cleaning labels\n","            valid_labels = ['e', 'n', 'c']\n","            data['label'] = data['label'].apply(lambda r: r if r in valid_labels else None)\n","            data = data.dropna(subset=['label'])\n","            data = data.reset_index(drop=True)\n","\n","            if 'label_map' in kwargs:\n","                data['label'] = data['label'].apply(lambda l: kwargs['label_map'][l])\n","\n","            premise_hypothesis, labels = data['premise_hypothesis'].values.tolist(), data['label'].values.tolist()\n","            print(f'test part:\\n #premise_hypothesis: {len(premise_hypothesis)}, #label: {len(labels)}')\n","            return premise_hypothesis, labels\n","        if dataset_name.lower() == \"farstail\":\n","            if not os.path.exists(dataset_file):\n","                print(f'{dataset_file} not exists!')\n","                return\n","            data = pd.read_csv(dataset_file, sep='\\t')\n","\n","            if 'label_map' in kwargs:\n","                data['label'] = data['label'].apply(lambda l: kwargs['label_map'][l])\n","\n","            premises = data['premise'].values.tolist()\n","            hypotheses = data['hypothesis'].values.tolist()\n","            labels = data['label'].values.tolist()\n","            print(f'test part:\\n #premise: {len(premises)}, #hypothesis: {len(hypotheses)}, #label: {len(labels)}')\n","            return premises, hypotheses, labels\n","\n","    def textual_entailment_inference(self, premises, hypotheses, device, max_length):\n","        if not self.model or not self.tokenizer or not self.id2label:\n","            print('Something wrong has been happened!')\n","            return\n","\n","        new_input = []\n","        for p, h in zip(premises, hypotheses):\n","            new_input.append((p, h))\n","\n","        tokenized_batch = self.tokenizer(\n","            new_input,\n","            padding=True,\n","            truncation=True,\n","            max_length=max_length,\n","            return_tensors=\"pt\"\n","        )\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","\n","        tokenized_batch = tokenized_batch.to(device)\n","        outputs = self.model(**tokenized_batch)\n","        pt_predictions = torch.argmax(F.softmax(outputs.logits, dim=1), dim=1)\n","\n","        output_predictions = []\n","        for i, premise in enumerate(premises):\n","            output_predictions.append(\n","                (premise, hypotheses[i], pt_predictions[i].item(), self.label_list[pt_predictions[i].item()])\n","            )\n","        return output_predictions\n","\n","    def mt5_textual_entailment_inference(self, premises, hypotheses, device):\n","        if not self.model or not self.tokenizer:\n","            print('Something wrong has been happened!')\n","            return\n","\n","        new_input = []\n","        for p, h in zip(premises, hypotheses):\n","            new_input.append(p + \"<sep>\" + h)\n","\n","        tokenized_batch = self.tokenizer(\n","            new_input,\n","            padding=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","\n","        input_ids = tokenized_batch.input_ids.to(device)\n","        attention_mask = tokenized_batch.attention_mask.to(device)\n","        outputs = self.model.generate(input_ids=input_ids,\n","                                      attention_mask=attention_mask)\n","        predictions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","        return predictions\n","\n","    def sentence_transformer_textual_entailment_inference(self, premises, hypotheses, device, label_map=None):\n","        if not self.model or not self.tokenizer:\n","            print('Something wrong has been happened!')\n","            return\n","        if label_map is None:\n","            label_map = {\"0<=score<0.4\": \"contradiction\", \"0.4<=score<=0.6\": \"neutral\", \"0.6<score<=1\": \"entailment\"}\n","            print(f\"Setting default value for label map: {label_map}\")\n","        elif not all('score' in cond for cond in label_map.keys()):\n","            print(f\"All the key of label_map must contain a condition on `score` variable.\\n\"\n","                  f\"For example: label_map ={label_map}\")\n","            return\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","\n","        premises_embeddings = self.model.encode(premises, convert_to_tensor=True, show_progress_bar=True)\n","        hypotheses_embeddings = self.model.encode(hypotheses, convert_to_tensor=True, show_progress_bar=True)\n","\n","        # Compute the pair-wise cosine similarities\n","        similarity_scores, predicted_labels = [], []\n","        for i in range(len(premises)):\n","            cosine_score = \\\n","                util.pytorch_cos_sim(premises_embeddings[i], hypotheses_embeddings[i]).cpu().detach().numpy()[0][0]\n","            similarity_scores.append(cosine_score)\n","            predicted_label = ''\n","            for exp in label_map:\n","                if eval(exp.replace('score', str(cosine_score))):\n","                    predicted_label = label_map[exp]\n","                    break\n","            predicted_labels.append(predicted_label)\n","\n","        output_predictions = []\n","        for i, premise in enumerate(premises):\n","            output_predictions.append(\n","                (premise, hypotheses[i], similarity_scores[i], predicted_labels[i])\n","            )\n","        return output_predictions\n","\n","    def evaluation(self, premise_hypothesis, labels, device, max_length, batch_size=4):\n","        if not self.model or not self.tokenizer or not self.id2label:\n","            print('Something wrong has been happened!')\n","            return\n","        label_count = {label: labels.count(label) for label in labels}\n","        print(\"label_count:\", label_count)\n","        dataset = TextualEntailmentDataset(premises_hypotheses=premise_hypothesis, targets=labels,\n","                                           label_list=self.label_list, tokenizer=self.tokenizer,\n","                                           model_type=self.model_type, max_length=max_length)\n","        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","        print(f'#premise_hypothesis:{len(premise_hypothesis)}, #labels:{len(labels)}')\n","        print(\"#batch:\", len(data_loader))\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","\n","        total_loss, total_time = 0, 0\n","        output_predictions = []\n","        golden_labels, predicted_labels = [], []\n","        print(\"Start to evaluate test data ...\")\n","        for step, batch in enumerate(data_loader):\n","            b_premises = batch['premise']\n","            b_input_ids = batch['input_ids']\n","            b_attention_mask = batch['attention_mask']\n","            b_token_type_ids = batch['token_type_ids']\n","            b_targets = batch['targets']\n","\n","            # move tensors to GPU if CUDA is available\n","            b_input_ids = b_input_ids.to(device)\n","            b_attention_mask = b_attention_mask.to(device)\n","            b_token_type_ids = b_token_type_ids.to(device)\n","            b_targets = b_targets.to(device)\n","\n","            # This will return the loss (rather than the model output) because we have provided the `labels`.\n","            with torch.no_grad():\n","                start = time.monotonic()\n","                b_outputs = self.model(input_ids=b_input_ids, attention_mask=b_attention_mask,\n","                                       token_type_ids=b_token_type_ids, labels=b_targets)\n","                end = time.monotonic()\n","                total_time += end - start\n","                print(f'inference time for step {step}: {end - start}')\n","            # get the loss\n","            total_loss += b_outputs.loss.item()\n","\n","            b_original_targets = batch['original_targets']\n","            golden_labels.extend(b_original_targets)\n","\n","            b_predictions = torch.argmax(F.softmax(b_outputs.logits, dim=1), dim=1)\n","            b_predictions = b_predictions.cpu().detach().numpy().tolist()\n","            b_predictions = [dataset.index2label[label] for label in b_predictions]\n","            predicted_labels.extend(b_predictions)\n","\n","            for i, premise in enumerate(b_premises):\n","                output_predictions.append((\n","                    premise,\n","                    batch['hypothesis'][i],\n","                    b_original_targets[i],\n","                    b_predictions[i]\n","                ))\n","\n","        # Calculate the average loss over the training data.\n","        avg_train_loss = total_loss / len(data_loader)\n","        print(\"average loss:\", avg_train_loss)\n","        print(\"total inference time:\", total_time)\n","        print(\"total inference time / #samples:\", total_time / len(premise_hypothesis))\n","\n","        # evaluate\n","        print(\"Test Accuracy: {}\".format(accuracy_score(golden_labels, predicted_labels)))\n","        print(\"Test Precision: {}\".format(precision_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test Recall: {}\".format(recall_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test F1-Score(weighted average): {}\".format(\n","            f1_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test classification Report:\\n{}\".format(classification_report(\n","            golden_labels, predicted_labels, digits=10)))\n","        return output_predictions\n","\n","    def evaluation_2(self, premises, hypotheses, labels, device, max_length, batch_size=4):\n","        if not self.model or not self.tokenizer or not self.id2label:\n","            print('Something wrong has been happened!')\n","            return\n","        label_count = {label: labels.count(label) for label in labels}\n","        print(\"label_count:\", label_count)\n","        dataset = TextualEntailmentDataset2(premises=premises, hypotheses=hypotheses, targets=labels,\n","                                            label_list=self.label_list, tokenizer=self.tokenizer,\n","                                            model_type=self.model_type, max_length=max_length)\n","        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","        print(f'#premises:{len(premises)}, #hypotheses:{len(hypotheses)}, #labels:{len(labels)}')\n","        print(\"#batch:\", len(data_loader))\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","\n","        total_loss, total_time = 0, 0\n","        output_predictions = []\n","        golden_labels, predicted_labels = [], []\n","        print(\"Start to evaluate test data ...\")\n","        for step, batch in enumerate(data_loader):\n","            b_premises = batch['premise']\n","            b_input_ids = batch['input_ids']\n","            b_attention_mask = batch['attention_mask']\n","            b_token_type_ids = batch['token_type_ids']\n","            b_targets = batch['targets']\n","\n","            # move tensors to GPU if CUDA is available\n","            b_input_ids = b_input_ids.to(device)\n","            b_attention_mask = b_attention_mask.to(device)\n","            b_token_type_ids = b_token_type_ids.to(device)\n","            b_targets = b_targets.to(device)\n","\n","            # This will return the loss (rather than the model output) because we have provided the `labels`.\n","            with torch.no_grad():\n","                start = time.monotonic()\n","                b_outputs = self.model(input_ids=b_input_ids, attention_mask=b_attention_mask,\n","                                       token_type_ids=b_token_type_ids, labels=b_targets)\n","                end = time.monotonic()\n","                total_time += end - start\n","                print(f'inference time for step {step}: {end - start}')\n","            # get the loss\n","            total_loss += b_outputs.loss.item()\n","\n","            b_original_targets = batch['original_targets']\n","            golden_labels.extend(b_original_targets)\n","\n","            b_predictions = torch.argmax(F.softmax(b_outputs.logits, dim=1), dim=1)\n","            b_predictions = b_predictions.cpu().detach().numpy().tolist()\n","            b_predictions = [dataset.index2label[label] for label in b_predictions]\n","            predicted_labels.extend(b_predictions)\n","\n","            for i, premise in enumerate(b_premises):\n","                output_predictions.append((\n","                    premise,\n","                    batch['hypothesis'][i],\n","                    b_original_targets[i],\n","                    b_predictions[i]\n","                ))\n","\n","        # Calculate the average loss over the training data.\n","        avg_train_loss = total_loss / len(data_loader)\n","        print(\"average loss:\", avg_train_loss)\n","        print(\"total inference time:\", total_time)\n","        print(\"total inference time / #samples:\", total_time / len(premises))\n","\n","        # evaluate\n","        print(\"Test Accuracy: {}\".format(accuracy_score(golden_labels, predicted_labels)))\n","        print(\"Test Precision: {}\".format(precision_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test Recall: {}\".format(recall_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test F1-Score(weighted average): {}\".format(\n","            f1_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test classification Report:\\n{}\".format(classification_report(\n","            golden_labels, predicted_labels, digits=10)))\n","        return output_predictions\n","\n","    def mt5_evaluation(self, premise_hypothesis, labels, device, max_length, batch_size=4):\n","        if not self.model or not self.tokenizer:\n","            print('Something wrong has been happened!')\n","            return\n","        if len(premise_hypothesis) != len(labels):\n","            print('length of inputs and labels is not equal!!')\n","            return\n","\n","        dataset = TextualEntailmentDataset(premises_hypotheses=premise_hypothesis, targets=labels,\n","                                           label_list=self.label_list, tokenizer=self.tokenizer,\n","                                           model_type=self.model_type, max_length=max_length)\n","        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","        print(f'#premise_hypothesis:{len(premise_hypothesis)}, #labels:{len(labels)}')\n","        print(\"#batch:\", len(data_loader))\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","\n","        total_time = 0\n","        output_predictions = []\n","        golden_labels, predicted_labels = [], []\n","        print(\"Start to evaluate test data ...\")\n","        for step, batch in enumerate(data_loader):\n","            b_premises = batch['premise']\n","            b_input_ids = batch['input_ids']\n","            b_attention_mask = batch['attention_mask']\n","\n","            # move tensors to GPU if CUDA is available\n","            b_input_ids = b_input_ids.to(device)\n","            b_attention_mask = b_attention_mask.to(device)\n","\n","            # This will return the loss (rather than the model output) because we have provided the `labels`.\n","            with torch.no_grad():\n","                start = time.monotonic()\n","                b_outputs = self.model.generate(input_ids=b_input_ids, attention_mask=b_attention_mask)\n","                end = time.monotonic()\n","                total_time += end - start\n","                print(f'inference time for step {step}: {end - start}')\n","\n","            b_original_targets = batch['original_targets']\n","            golden_labels.extend(b_original_targets)\n","\n","            b_predictions = self.tokenizer.batch_decode(b_outputs, skip_special_tokens=True)\n","            predicted_labels.extend(b_predictions)\n","\n","            for i, premise in enumerate(b_premises):\n","                output_predictions.append((\n","                    premise,\n","                    batch['hypothesis'][i],\n","                    b_original_targets[i],\n","                    b_predictions[i]\n","                ))\n","\n","        print(\"total inference time:\", total_time)\n","        print(\"total inference time / #samples:\", total_time / len(premise_hypothesis))\n","\n","        # evaluate\n","        print(\"Test Accuracy: {}\".format(accuracy_score(golden_labels, predicted_labels)))\n","        print(\"Test Precision: {}\".format(precision_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test Recall: {}\".format(recall_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test F1-Score(weighted average): {}\".format(\n","            f1_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test classification Report:\\n{}\".format(\n","            classification_report(golden_labels, predicted_labels, digits=10)))\n","        return output_predictions\n","\n","    def evaluation_pair_similarity(self, premises, hypotheses, labels, device, label_map=None, batch_size=4):\n","        if not self.model or not self.tokenizer:\n","            print('Something wrong has been happened!')\n","            return\n","        if label_map is None:\n","            label_map = {\"0<=score<0.4\": \"contradiction\", \"0.4<=score<=0.6\": \"neutral\",\n","                         \"0.6<score<=1\": \"entailment\"}\n","            print(f\"Setting default value for label map: {label_map}\")\n","        elif not all('score' in cond for cond in label_map.keys()):\n","            print(f\"All the key of label_map must contain a condition on `score` variable.\\n\"\n","                  f\"For example: label_map ={label_map}\")\n","            return\n","\n","        label_count = {label: labels.count(label) for label in labels}\n","        print(\"label_count:\", label_count)\n","        print(f'#premises:{len(premises)}, #hypotheses:{len(hypotheses)}, #labels:{len(labels)}')\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","\n","        total_time = 0\n","        print(\"Start to evaluate test data ...\")\n","\n","        # Compute the sentence embeddings\n","        start = time.monotonic()\n","        premises_embeddings = self.model.encode(premises, convert_to_tensor=True, show_progress_bar=True,\n","                                                batch_size=batch_size)\n","        hypotheses_embeddings = self.model.encode(hypotheses, convert_to_tensor=True, show_progress_bar=True,\n","                                                  batch_size=batch_size)\n","        end = time.monotonic()\n","        print(f'time for computing embeddings of premises and hypotheses: {end - start}')\n","        total_time += end - start\n","\n","        dataset = TextualEntailmentSimilarityDataset(premises=premises_embeddings, hypotheses=hypotheses_embeddings,\n","                                                     targets=labels)\n","        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","        print(\"#batch:\", len(data_loader))\n","\n","        output_predictions = []\n","        golden_labels, predicted_labels = [], []\n","        for step, batch in enumerate(data_loader):\n","            b_premises = batch['premise']\n","            b_hypotheses = batch['hypothesis']\n","\n","            # move tensors to GPU if CUDA is available\n","            b_premises = b_premises.to(device)\n","            b_hypotheses = b_hypotheses.to(device)\n","\n","            # Compute the pair-wise cosine similarities\n","            start = time.monotonic()\n","            cos_similarity_scores, b_predictions = [], []\n","            for i in range(len(b_premises)):\n","                cosine_score = util.pytorch_cos_sim(b_premises[i], b_hypotheses[i]).cpu().detach().numpy()[0][0]\n","                cos_similarity_scores.append(cosine_score)\n","                predicted_label = ''\n","                for exp in label_map:\n","                    if eval(exp.replace('score', str(cosine_score))):\n","                        predicted_label = label_map[exp]\n","                        break\n","                b_predictions.append(predicted_label)\n","\n","            end = time.monotonic()\n","            total_time += end - start\n","            print(f'time for calculating cosine similarity in step {step}: {end - start}')\n","\n","            golden_labels.extend(batch['targets'])\n","            predicted_labels.extend(b_predictions)\n","\n","            for i, item in enumerate(batch['item']):\n","                output_predictions.append((\n","                    premises[item],\n","                    hypotheses[item],\n","                    cos_similarity_scores[i],\n","                    batch['targets'][i],\n","                    b_predictions[i]\n","                ))\n","\n","        print(\"total inference time:\", total_time)\n","        print(\"total inference time / #samples:\", total_time / len(premises))\n","\n","        # evaluate\n","        print(\"Test Accuracy: {}\".format(accuracy_score(golden_labels, predicted_labels)))\n","        print(\"Test Precision: {}\".format(precision_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test Recall: {}\".format(recall_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test F1-Score(weighted average): {}\".format(\n","            f1_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test classification Report:\\n{}\".format(classification_report(\n","            golden_labels, predicted_labels, digits=10)))\n","        return output_predictions\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":782,"referenced_widgets":["d604494f9acf4342bf26f42b78c2570a","898f5f6d430e40b2bebf610a7c489850","e688c77d8a8c441292ef18381d15efc7","28fdb2a436844380b33f954a8c0a6440","5f034ccd936f4d62a421cadf46211bb3","79a870f79247473293615daf1d4f46f7","0ad68ef6af2f41d3a4b57115f17d7948","b49e23ac620f460ea93bf0dc1d7705d7","90aa5816ed224998bb05c2114da4f3c2","929bf109b6784deda1bfcd9e3aac87e3","82bf0ffa71c2478e9652246adea41778","9fd2b4af1a86406986a2e012ce09a47e","04b9fe96203548b68152e59b02ba3e55","76df09b4a901419aa5c2e24663b8ff54","1340cd2705694e498ef875e65eca3d04","0bdbf43090524f2da0918dddb2bdf0c2","587aa494792249c6a7edc31f4c9b184d","74756b5f9ea94cfca668ce854f6b8e92","e8d09795a7d8435da93e0060245d3085","a174035594474b4aaa1429f9a5dfc962","158eeeadce7348f8950fc667e5189d53","8672001d2f434214a3f4299b912bb148","c97d297db279432f84935339c321410e","47801d0e71a1435385c3169726d8821b","f5b2b98a20fc408ba3d6d9aab03d1178","c304b1f832084cb3a38b256a2a1f30e5","e8b6cdae65b04c7298194c679154bca6","6e91e1faf64e4c7091176886a578cc23","d9fdca1317cd49e5906a542a2163dfb6","f0f5de7e6a3643c5bfec7203943e92a3","e3a59ef75d0b4f1ab41a24d5208321bd","6a5bef87d8bb46f48f15c18eeafcce17","d925eb994be34f8891e142ab1f62a588","5b5a6fd95e1e4ecb9d35e0ce2f982708","5ed034cbdd8f4ce987d5995774036206","c58af0b899ae4a8287cb0737e6b21d5f","7b644720910e4eeebf4b50d27d5e8bcd","867f9634a1ca4c62b31a1a0b625b9965","f31a9636b5984415bd8a16034ead29e4","7ac4a7405b2d408a9f1e0134793a178c"]},"id":"LAPBCpMo_mNB","executionInfo":{"status":"ok","timestamp":1628065013720,"user_tz":-270,"elapsed":228962,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"bb8aeb1d-75c9-4a76-e02e-43046983647b"},"source":["model_name='persiannlp/mt5-base-parsinlu-snli-entailment'\n","te_model = TextualEntailment(model_name=model_name, model_type=\"mt5\", label_list = ['e', 'c', 'n'])\n","print(te_model.config)"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d604494f9acf4342bf26f42b78c2570a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=4309802.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90aa5816ed224998bb05c2114da4f3c2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=65.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"587aa494792249c6a7edc31f4c9b184d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=375.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5b2b98a20fc408ba3d6d9aab03d1178","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=695.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["You are using a model of type t5 to instantiate a model of type mt5. This is not supported for all configurations of models and can yield errors.\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d925eb994be34f8891e142ab1f62a588","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2329703923.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["You are using a model of type t5 to instantiate a model of type mt5. This is not supported for all configurations of models and can yield errors.\n"],"name":"stderr"},{"output_type":"stream","text":["MT5Config {\n","  \"_name_or_path\": \"/home/patrick/hugging_face/t5/mt5-base\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.7.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wQzuLKPi-Y4f"},"source":["## Sample Inference"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3QlUxWGK_TL","executionInfo":{"status":"ok","timestamp":1628065025003,"user_tz":-270,"elapsed":11298,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"d80b3090-c7ab-403a-8398-6ee5358555c1"},"source":["premise_list = [\n","    \"این مسابقات بین آوریل و دسامبر در هیپودروم ولیفندی در نزدیکی باکرکی ، ۱۵ کیلومتری (۹ مایل) غرب استانبول برگزار می شود.\",\n","    \"آیا کودکانی وجود دارند که نیاز به سرگرمی دارند؟\",\n","    \"ما به سفرهایی رفته ایم که در نهرهایی شنا کرده ایم\"\n","]\n","hypothesis_list = [\n","    \"در ولیفندی هیپودروم، مسابقاتی از آوریل تا دسامبر وجود دارد.\",\n","    \"هیچ کودکی هرگز نمی خواهد سرگرم شود.\",\n","    \"علاوه بر استحمام در نهرها ، ما به اسپا ها و سونا ها نیز رفته ایم.\"\n","]\n","te_model.mt5_textual_entailment_inference(premise_list, hypothesis_list, device)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['e', 'c', 'n']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"KhzQ5QWA-cB4"},"source":["## ParsiNLU Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HC8SZ9uXK_V9","executionInfo":{"status":"ok","timestamp":1628065031923,"user_tz":-270,"elapsed":6924,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"89dd93b8-5201-46b9-bb0a-5e16c38ce735"},"source":["!git clone https://github.com/persiannlp/parsinlu\n","!ls parsinlu\n","!ls parsinlu/data/entailment/merged_with_farstail"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Cloning into 'parsinlu'...\n","remote: Enumerating objects: 1434, done.\u001b[K\n","remote: Counting objects: 100% (182/182), done.\u001b[K\n","remote: Compressing objects: 100% (98/98), done.\u001b[K\n","remote: Total 1434 (delta 110), reused 139 (delta 82), pack-reused 1252\u001b[K\n","Receiving objects: 100% (1434/1434), 27.81 MiB | 12.88 MiB/s, done.\n","Resolving deltas: 100% (913/913), done.\n","data  LICENSE  README.md  requirements.txt  scripts  src\n","dev.tsv  test_farstail.tsv  test_natural.tsv  test_translation.tsv  train.tsv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"skr7hrVuADX3"},"source":["### parsinlu natural subset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"df8OWH6jK_Yq","executionInfo":{"status":"ok","timestamp":1628065032519,"user_tz":-270,"elapsed":608,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"d653dca3-7980-4717-cbae-94de43071ea6"},"source":["test_natural, test_labels = te_model.load_dataset_test_file(dataset_name=\"parsinlu-natural\", dataset_file=\"./parsinlu/data/entailment/merged_with_farstail/test_natural.tsv\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["test part:\n"," #premise_hypothesis: 850, #label: 850\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Jzd13_wp-85K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628065032519,"user_tz":-270,"elapsed":9,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"c8c295f8-0f77-48cd-ad35-af813e9a2084"},"source":["!nvidia-smi\n","!lscpu"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Wed Aug  4 08:17:14 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    54W / 149W |   2781MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Architecture:        x86_64\n","CPU op-mode(s):      32-bit, 64-bit\n","Byte Order:          Little Endian\n","CPU(s):              2\n","On-line CPU(s) list: 0,1\n","Thread(s) per core:  2\n","Core(s) per socket:  1\n","Socket(s):           1\n","NUMA node(s):        1\n","Vendor ID:           GenuineIntel\n","CPU family:          6\n","Model:               79\n","Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n","Stepping:            0\n","CPU MHz:             2199.998\n","BogoMIPS:            4399.99\n","Hypervisor vendor:   KVM\n","Virtualization type: full\n","L1d cache:           32K\n","L1i cache:           32K\n","L2 cache:            256K\n","L3 cache:            56320K\n","NUMA node0 CPU(s):   0,1\n","Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HMzjyG2t_ybv","executionInfo":{"status":"ok","timestamp":1628065101821,"user_tz":-270,"elapsed":68844,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"18e6130b-e9c2-45b6-8147-e92ab4e3ef59"},"source":["evaluation_output = te_model.mt5_evaluation(test_natural, test_labels, device, max_length=512, batch_size=128)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["#premise_hypothesis:850, #labels:850\n","#batch: 7\n","Start to evaluate test data ...\n","inference time for step 0: 10.427790899000001\n","inference time for step 1: 10.253662849999955\n","inference time for step 2: 10.276047062000089\n","inference time for step 3: 10.27092909299995\n","inference time for step 4: 10.32053159599991\n","inference time for step 5: 10.369510639000055\n","inference time for step 6: 6.702110466000022\n","total inference time: 68.62058260499998\n","total inference time / #samples: 0.08073009718235293\n","Test Accuracy: 0.5776470588235294\n","Test Precision: 0.5732101314633359\n","Test Recall: 0.5776470588235294\n","Test F1-Score(weighted average): 0.5723369012360874\n","Test classification Report:\n","              precision    recall  f1-score   support\n","\n","           c  0.5280373832 0.4396887160 0.4798301486       257\n","           e  0.5798045603 0.5579937304 0.5686900958       319\n","           n  0.6079027356 0.7299270073 0.6633499171       274\n","\n","    accuracy                      0.5776470588       850\n","   macro avg  0.5719148930 0.5758698179 0.5706233872       850\n","weighted avg  0.5732101315 0.5776470588 0.5723369012       850\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9eDgPzwO_rek","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628065101822,"user_tz":-270,"elapsed":17,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"e5299c77-0731-4c95-9c3f-2a28896061b2"},"source":["for premise, hypothesis, true_label, predicted_label in evaluation_output[:25]:\n","  print('{}\\t{}\\t{}\\t{}'.format(premise, hypothesis, true_label, predicted_label))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["او را نیز بکشتند و پس از او، هیچ‌کس را نیافتند که شایسته پادشاهی باشد.\tاو را نیز بکشتند و پس از او پسرش را شایسته ترین فرد برای پادشاهی یافتند.\tc\te\n","نعمت‌زاده همچنین با اشاره به ارتقاء کیفیت خودروهای داخلی طی چند سال گذشته، گفت: با اقدامات صورت گرفته کیفیت خودروهای داخلی رو به بهبود است، اما تا وضع مطلوب هنوز فاصله داریم.\tبه گفته‌ی وی تولید خودرو در مقایسه با سال‌های گذشته افزایش یافته است.\tn\tc\n","صابر دین‌پژوه‌ رتبه اول علوم ریاضی و فنی کنکور سال 96 که از شهر تبریز حائز این رتبه شده بود در مورد موفقیت خود اظهار کرد: پیش از اعلام نتایج و با احتساب درصد‌های کنکور حدس می‌زدم که یکی از رتبه‌های برتر و تک رقمی کنکور را کسب کنم زیرا در کنکورهای‌ آزمایشی در طول سال تحصیلی همواره‌ رتبه‌های تک رقمی و برتر را کسب می‌کردم.\tرتبه‌ی اول علوم ریاضی و فنی کنکور از نتیجه‌ی کنکور خود متعجب بود. \tc\tc\n","وی پس از رسیدن به کامپیوتر اصلی وارد ماتریکس شده و نبرد سختی بین او و اسمیت رخ می‌دهد.\tدر این فیلم شبکه های اینترنتی که در آن زمان پدیدۀ جدیدی بود بخوبی تصویر شده است.\tn\tn\n","به‌طور کلی سلسه مراتب و تقسیم‌بندی حیاط بر اساس فعالیت‌ها صورت می‌گیرد.\tانسان خانه را میسازد و خانه انسان را.\tn\tn\n","در شهر برلین هانری ربن قرار است با شخصی به نام پس یرگارن دیدار کند.\tدر شهر برلین پس یرگارن قرار است با شخصی به نام هانری ربن دیدار کند.\te\te\n","او پس از پیدا کردن اشلی در پایان مجبور می‌شود تا با سدلر مبارزه کند.\tافسانه ها نقش مهمی در زندگی ما دارند و نباید دست کم گرفته شوند. \tn\tn\n","بیشتر انواع خرس همه‌چیزخوار به‌شمار می‌آیند، ولی رژیم غذایی هر یک از افراد آن‌ها ممکن است از منحصراً گوشت‌خوار تا منحصراً گیاه‌خوار متفاوت باشد که این بستگی به آن دارد که چه نوع منابع غذایی در هر محل یا هر فصل در دسترس آن‌ها قرار دارد.\tهمه خرس ها گوشتخوارند\tc\tc\n","داریوش فرمان داد تا کار دوباره آغاز شود و بر سر راه چاه‌هایی کنده شوند تا آب آشامیدنی برای کارگران بدست آید.\tفرمان قطع آب به روی کارگران توسط داریوش صادر شد.\tc\te\n","این نقل قول که: ”من کور بودم، ولی اکنون می‌بینم” حالا مفهوم جدیدی برایم پیدا کرده‌است. حالا می‌فهمم که ما در زندگی زمینی‌مان تا چه حد در مورد کیفیت واقعی عوالم معنوی کور هستیم، به خصوص افرادی مثل من که در گذشته، ماده را هستهٔ مرکزی واقعیت می‌پنداشتم و چیزهای دیگر- فکر، آگاهی، ایده‌ها، عواطف، روح- را فقط حاصل برهمکنش ماده تصور می‌کردم.\tاکنون چیزهایی را درک می کنم که قبلا آن ها را درک نمی‌کردم.\te\tn\n","او معتقد است اسلام با ضابطه مند کردن چند همسری مردان، به نفع زنان وارد شد.\tباید نظر زنان را هم در اینباره پرسید.\tn\tn\n","معمولا در این نوع ازدواج ها والدین تفاوت ها و مشکلاتی را می بینند که کاملا براساس منطق و عقل است اما بیان کردن این موارد برای فرزندشان که عاشق است، فایده زیادی ندارد چرا که معمولا جوان های امروزی منطق و طرز تفکر والدین خود را نمی پذیرند و مشکلاتی که آنها امروز پیش بینی می کنند، اصلا باور ندارد و قبول نمی کنند.\tتلاش والدین برای بیان مشکلات و تفاوت‌هایی که وجود دارد، بی نتیجه است.\te\te\n","به این توجه داشته باشید که در ساختار سوم یا همان ساختار کمپلکس فقط یک پروتئین به دور خود می پیچد ولی در ساختار چهارم چند پروتئین به دور هم میپیچند.\tبه این توجه داشته باشید که در ساختار سوم یا همان ساختار کمپلکس چند پروتئین به دور خود می پیچد ولی در ساختار چهارم یک پروتئین به دور هم میپیچند.\tc\te\n","یاری تزابان که چهار دوره عضو مجلس اسرائیل بوده‌است می‌گوید که پس از قتل‌عام همراه با بریگاردهای جوان به روستا ارسال شده بود تا جسدها را دفن کند.\tقتل عام در روستا طوری بود که هم از نظامیان کشته شده بودند و هم از غیر نظامیان.\tn\tn\n","اما با دسته‌بندی این اعداد باینری بزرگ به گروه‌های کوچکتر و با تعداد ارقام مساوی، فهم و نوشتن آنها راحت‌تر خواهد بود.\tاعداد باینری بزرگ پس از دسته بندی به گروه های کوچک ، وارد محاسبات می شوند.\tn\tn\n","او پس از سال‌ها به ایران بازمی‌گردد و به دنبال دختر گمشده خود می‌گردد و متوجه می‌شود که نادر و زیور، خواهرزاده و همسر خواهرزاده‌اش، نتوانسته‌اند از دخترش نگهداری کنند و دخترش از خانه به دلیل آزار و اذیت آن‌ها فرار کرده و میان گروه های تکدی‌گری سر درآورده است.\tدختر او به دلیل آزار و اذیت نادر و زیور، از خانه فرار کرده است\te\te\n","د ما این نامه را از جمله به بانوئی که باصطلاح کمیسر، یاهمان ترتیب دهنده نمایشگاه است، وسرپرستی کار را به عهده دارد ، و نيز برای رئیس موزه لوورفرستادیم، ولی برای شما گفته باشم که پاسخی هم دریافت نکردیم .\tموزه لوور به نامه ما سریعا پاسخ داد\tc\tc\n","وی در ۱۹ اکتبر ۱۹۵۷ در حالی دنیا را وداع گفت که پس از بازنشستگی از دانشگاه لندن قصد نگارش کتابی دیگر را داشت که اجل به او فرصت این کار را نداد.\tآخرین کتاب او در دانشگاه لندن نگارش شد.\tn\te\n","این درگیری پس از جنگ جهانی اول و قبل از جنگ جهانی دوم صورت گرفت.\tدرگیری ها پیش از جنگ جهانی اول به پایان رسید.\tc\te\n","روش تولید این گاز از طریق یک سیلندر یا کمپرسور می‌باشد.\tگاز مورد استفاده میتواند هر گاز دیگری باشد.\tn\tn\n","مرد جنایتکار قبل از این که بتواند جسد را از مخفیگاهش خارج کند، توسط پلیس دستگیر شده بود.\tقاتل جسد را به خارج شهر برد و دفن کرد اما پس از آن توسط پلیس دستگیر شد. \tc\te\n","این انرژی همان آنتالپی استاندارد است هنگامی که یک پیوند با کمک همکافت شکسته شود و محصولات همکافت در صفر مطلق به دست آید.\tبا شکست پیوندها انرژی آزاد میشود. \tn\tc\n","آلفونسو متوجه می‌شود که ترس و وحشت تمام وجود دو خواهر را فراگرفته است.  احساس آنها این بود که فراندو و گولیئلمو در چند قدمی خانه هستند.\tآلفونسو که ترس آنها را متوجه شده بود به آنها پیشنهاد جالبی داد. \te\tn\n","ممکن است عملگرها فقط توان باز یا بسته کردن کامل شیر را داشته باشند؛ اما در بعضی عملگرها این امکان وجود دارد که موقعیت دقیق شیر را تنظیم کرد.\tنوعی از عملگرها قابلیت برخی از تنظیمات دقیقتر را دارا هستند.\te\te\n","به‌طور متوسط استرالیا تنها یک نیش مار کشنده در هر سال دارد.\tدر استرالیا ۲۵۰ هزار مارگزیدگی در سال ثبت می شود و از آن‌ها ۵۰ هزار منجر به مرگ می‌شود.\tc\tn\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fxlJnN9vAKKP","executionInfo":{"status":"ok","timestamp":1628065104496,"user_tz":-270,"elapsed":2119,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}}},"source":["output_file_name = \"textual_entailment-parsinlu-natural_{}_outputs.txt\".format(model_name.replace('/','-'))\n","with open(output_file_name, \"w\", encoding='utf8') as output_file:\n","  for premise, hypothesis, true_label, predicted_label in evaluation_output:\n","    output_file.write('{}\\t{}\\t{}\\t{}\\n'.format(premise, hypothesis, true_label, predicted_label))\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","upload = drive.CreateFile({'title': output_file_name})\n","upload.SetContentFile(output_file_name)\n","upload.Upload()"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qj8lPy-_AMOh"},"source":["### parsinlu mnli subset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aeWIvpVf7fUL","executionInfo":{"status":"ok","timestamp":1628065104496,"user_tz":-270,"elapsed":14,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"68d77d3f-b2de-4ae6-e423-fcf2c954a325"},"source":["test_mnli, test_labels = te_model.load_dataset_test_file(dataset_name=\"parsinlu-mnli\", dataset_file=\"./parsinlu/data/entailment/merged_with_farstail/test_translation.tsv\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["test part:\n"," #premise_hypothesis: 823, #label: 823\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GPqXPCnp-8YE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628065104955,"user_tz":-270,"elapsed":471,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"de33ef12-8485-4510-eb84-5c0f0ae302c1"},"source":["!nvidia-smi\n","!lscpu"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Wed Aug  4 08:18:26 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   66C    P0    64W / 149W |   8931MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Architecture:        x86_64\n","CPU op-mode(s):      32-bit, 64-bit\n","Byte Order:          Little Endian\n","CPU(s):              2\n","On-line CPU(s) list: 0,1\n","Thread(s) per core:  2\n","Core(s) per socket:  1\n","Socket(s):           1\n","NUMA node(s):        1\n","Vendor ID:           GenuineIntel\n","CPU family:          6\n","Model:               79\n","Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n","Stepping:            0\n","CPU MHz:             2199.998\n","BogoMIPS:            4399.99\n","Hypervisor vendor:   KVM\n","Virtualization type: full\n","L1d cache:           32K\n","L1i cache:           32K\n","L2 cache:            256K\n","L3 cache:            56320K\n","NUMA node0 CPU(s):   0,1\n","Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NYh_Tq4Q--Ok","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628065173814,"user_tz":-270,"elapsed":68861,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"ed89e2f3-e9f9-4e9c-ec8c-c54d29f9a4d1"},"source":["evaluation_output = te_model.mt5_evaluation(test_mnli, test_labels, device, max_length=512, batch_size=128)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["#premise_hypothesis:823, #labels:823\n","#batch: 7\n","Start to evaluate test data ...\n","inference time for step 0: 10.444368700000268\n","inference time for step 1: 10.431672344000162\n","inference time for step 2: 10.459336808999979\n","inference time for step 3: 10.466134346000217\n","inference time for step 4: 10.46667309600025\n","inference time for step 5: 10.455306785000175\n","inference time for step 6: 4.507054659999994\n","total inference time: 67.23054674000105\n","total inference time / #samples: 0.08168960721749824\n","Test Accuracy: 0.6281895504252734\n","Test Precision: 0.62789457823887\n","Test Recall: 0.6281895504252734\n","Test F1-Score(weighted average): 0.628032035652199\n","Test classification Report:\n","              precision    recall  f1-score   support\n","\n","           c  0.6307189542 0.6348684211 0.6327868852       304\n","           e  0.6746575342 0.6769759450 0.6758147513       291\n","           n  0.5644444444 0.5570175439 0.5607064018       228\n","\n","    accuracy                      0.6281895504       823\n","   macro avg  0.6232736443 0.6229539700 0.6231026794       823\n","weighted avg  0.6278945782 0.6281895504 0.6280320357       823\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DC5-m0I__aHZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628065173814,"user_tz":-270,"elapsed":14,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"d91d9e04-665f-41ea-9c60-f34a9b2a3146"},"source":["for premise, hypothesis, true_label, predicted_label in evaluation_output[:25]:\n","  print('{}\\t{}\\t{}\\t{}'.format(premise, hypothesis, true_label, predicted_label))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["آنها به عنوان (الف) رسید جبران خسارت طبقه بندی می شوند (یعنی\tچیز خوبی است که آنها در این دسته قرار می گیرند.\tn\tn\n","مورد دیگری که باید ببینید ، فیلم خوابیدن با دشمن است.\tباید خوابیدن با دشمن را ببینی.\te\te\n","داستانهای افسانه ای ارزش خود را دارند ، اما پنهان کاری کثیف است.\tپنهان کردن چیزها فقط کثیف است ، در حالی که در داستان افسانه ای، جلال وجود دارد\te\tc\n","او یک دستش را به سمت سمندر آورد ، آن را به آرامی نوازش کرد و آن را دوباره روی سینه دیو گذاشت.\tسمندر پذیرای حرکات دوستانه او بود.\tn\te\n","بنابراین ، با دقت بسیار زیاد ، افشاگری نهایی فیلم Zapruder این است که ابراهیم Zapruder خود توطئه گر بود.\tدر فیلم Zapruder فاش می شود که او توطئه گر بود ، این برای عموم شوک آور بود.\tn\tn\n","ایده خوبی هست\tبهترین ایده ای بود که من شنیده ام.\tn\tn\n","جان ، در حالی که هنوز هم به چشمان آدرین نگاه می کرد ، دید که یک تیغه بسیار تیز قفسه سینه کال را شکافت .\tتیغی از سینه کال عبور کرد.\te\te\n","برای به دست آوردن یک تجربه‌ی کامل ، در غروب آفتاب یا خارج از فصل بازدید کنید.\tبهتر است در یکشنبه آفتابی یا در فصل کم بازدید کنید.\te\tc\n","می توانید در دیواره های سنگی ، سرهای کمی فرشته از فرشتگان یا شیاطین، نقوش گل و یا گل صدفی (کوکی سن ژاک) را ببینید که مسیر زائران قرون وسطایی را به سانتیاگو د کامپوستلا در اسپانیا نشان می دهد.\tمسیر زائران قرون وسطایی به سانتیاگو د کامپوستلا اسپانیا را به آلمان وصل می کند.\tn\tc\n","حامد نوروزی در سن ۳۹ سالگی شروع به راه اندازی کسب و کار خود کرد.\tشخصی به نام حامد نوروزی حداقل ۳۹ سال عمر کرد.\te\te\n","شما می دانید که این کار را انجام دهید و این واقعا بعد از مدتی به نوعی بدتر می شود ، زیرا می گویید من تمام وقت هم کار می کنم\tشما همچنین تمام وقت کار می کنید و این بدان معنی است که شما بسیار سرتان شلوغ است.\tn\tc\n","بله٬ هوم اینجا به نحوی عجیب است و نحوی که مسائل انجام میشوند. در اینجا اگر شما تصادف کنید اما کسی زخمی نشود پلیس نمی آید. \tاگر کسی مجروح نشده باشد ، پلیس نمی آید.\te\te\n","آن اتفاق با ظهور هواپیماهای مسافربری و کشتی های دریایی مسیر شد.\tهیچ کشتی کروزی در منطقه وجود ندارد.\tc\tc\n","همه باید سگ بزنند شما باید بیرون بروید باید قدم بزنید و بعد شروع کردم به شنیدن در مورد٬ خوب من قادر به قدم زدن نیستم زیرا من آسم دارم.\tمن آسم دارم و نمی توانم کار کنم.\te\te\n","بله بله ، وقت زیادی را در شارلوت سپری می کنید و فقط در تلویزیون معمولی که کابلی نیست ، می توانید چهار کانال PB یا سه کانال PBS را انتخاب کنید.\tهمه نوع دسترسی کابلی به کانال را در شارلوت می توانید دریافت کنید.\tc\tn\n","شما وارد دروازه واگن می شوید ، جایی که ، در موارد نادری که به زنان اجازه خروج داده می شد، زنان سوار واگن های خود می شدند.\tزنان اغلب بدون پوشیدن لباس مناسب مجاز به بیرون رفتن نبودند.\tn\tn\n","کمی دورتر از زادگاه ژوزفین زمین گلفی وجود داشت.\tگویی جوزفین برای تاسیس یک باشگاه گلف متولد شده بود.\tn\tn\n","آیا فکر می کنید خانم اینگلوتورپ وصیت نامه ای کرد که تمام پول خود را به دست خانم هوارد سپرد؟ با صدایی آرام و کمی کنجکاوی پرسیدم.\tبا تمام وجود فریاد زدم.\tc\tc\n","گوش کن ، آقا کربی ، اگر با ریبوزها سوار شوید ، بهتر است وقتی که ضربه آبی رنگ در شهر برخورد کرد ، لب خود را به حالت ایستاده نگه دارید.\tشما سوار Rebs شدید و این من را بسیار عصبانی می کند!\tn\tc\n","جایی که بیش از یک خانواده در یک خانه هستند.\tوقتی بیش از یک خانواده در یک خانه هستند.\te\te\n","شرکت کنندگان عموماً موافق بودند که پیشرفت در مدیریت شرکتها باعث بهبود در حسابرسی خواهد شد.\tشرکت کنندگان فقط در مورد چند موضوع به توافق رسیدند.\tn\tc\n","چگونه و چه کاری انجام می دهید؟\tچیکار میکنی؟\te\te\n","خروجی های شبکه وارد شده است.\tنحوه ی کارکرد شبکه برای عموم اطلاع رسانی نشده است.\tn\tn\n","یک بار ، نمایشگاه های لاس وگاس پر از سرفصل های سرگرمی برتر ، طنزپردازان ، نمایش های تولیدی و دختران رقاص بود که می توان با قیمت بسیار کمی از آنها لذت برد.\tبا توجه به تورم و ارزش تولید ، اکنون نمایش های لاس وگاس برای دیدن گران تر هستند.\tn\te\n","دین گلن گفت: با پیشنهادات دانشگاه های حقوقی با بسته های مالی در حال افزایش که اکثر آن قرضی است، به نوعی جنگ پیشنهادات در حال وقوع است.\tدر واقع هیچ کس به این که دانشجویان جدیدی به موسساتشان بیایند، اهمیتی نمی دهد.\tc\tn\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bpmkFnP4AHOK","executionInfo":{"status":"ok","timestamp":1628065176178,"user_tz":-270,"elapsed":2376,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}}},"source":["output_file_name = \"textual_entailment-parsinlu-mnli_{}_outputs.txt\".format(model_name.replace('/','-'))\n","with open(output_file_name, \"w\", encoding='utf8') as output_file:\n","  for premise, hypothesis, true_label, predicted_label in evaluation_output:\n","    output_file.write('{}\\t{}\\t{}\\t{}\\n'.format(premise, hypothesis, true_label, predicted_label))\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","upload = drive.CreateFile({'title': output_file_name})\n","upload.SetContentFile(output_file_name)\n","upload.Upload()"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BaeCr95hAi75"},"source":["### parsinlu farstail subset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Zp5pRsS7fYR","executionInfo":{"status":"ok","timestamp":1628065176179,"user_tz":-270,"elapsed":11,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"ab2e3860-b0ab-4d5d-8134-348df8747c2d"},"source":["test_farstail, test_labels = te_model.load_dataset_test_file(dataset_name=\"parsinlu-farstail\", dataset_file=\"./parsinlu/data/entailment/merged_with_farstail/test_farstail.tsv\")"],"execution_count":19,"outputs":[{"output_type":"stream","text":["test part:\n"," #premise_hypothesis: 1564, #label: 1564\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4knw0YgC0SfX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628065176180,"user_tz":-270,"elapsed":11,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"1a9d3c92-7003-424d-a84d-0a9137afaa1d"},"source":["!nvidia-smi\n","!lscpu"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Wed Aug  4 08:19:37 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   71C    P0    66W / 149W |   8931MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Architecture:        x86_64\n","CPU op-mode(s):      32-bit, 64-bit\n","Byte Order:          Little Endian\n","CPU(s):              2\n","On-line CPU(s) list: 0,1\n","Thread(s) per core:  2\n","Core(s) per socket:  1\n","Socket(s):           1\n","NUMA node(s):        1\n","Vendor ID:           GenuineIntel\n","CPU family:          6\n","Model:               79\n","Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n","Stepping:            0\n","CPU MHz:             2199.998\n","BogoMIPS:            4399.99\n","Hypervisor vendor:   KVM\n","Virtualization type: full\n","L1d cache:           32K\n","L1i cache:           32K\n","L2 cache:            256K\n","L3 cache:            56320K\n","NUMA node0 CPU(s):   0,1\n","Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T_ys5gRn--k_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628065306283,"user_tz":-270,"elapsed":130109,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"bb8278bf-87ef-473e-b65b-37110e6a0db4"},"source":["evaluation_output = te_model.mt5_evaluation(test_farstail, test_labels, device, max_length=512, batch_size=128)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["#premise_hypothesis:1564, #labels:1564\n","#batch: 13\n","Start to evaluate test data ...\n","inference time for step 0: 10.496816194000075\n","inference time for step 1: 10.49263314100017\n","inference time for step 2: 10.50058713899989\n","inference time for step 3: 10.501429460000054\n","inference time for step 4: 10.4924952639999\n","inference time for step 5: 10.49634426700004\n","inference time for step 6: 10.504555814000014\n","inference time for step 7: 10.51602919600009\n","inference time for step 8: 10.508996875999856\n","inference time for step 9: 10.492404059999899\n","inference time for step 10: 10.51030880999997\n","inference time for step 11: 10.51732290100017\n","inference time for step 12: 2.3566315369998847\n","total inference time: 128.386554659\n","total inference time / #samples: 0.08208858993542201\n","Test Accuracy: 0.8574168797953964\n","Test Precision: 0.8581977793662486\n","Test Recall: 0.8574168797953964\n","Test F1-Score(weighted average): 0.8576500591586295\n","Test classification Report:\n","              precision    recall  f1-score   support\n","\n","           c  0.8046875000 0.8078431373 0.8062622309       510\n","           e  0.8523364486 0.8786127168 0.8652751423       519\n","           n  0.9148936170 0.8841121495 0.8992395437       535\n","\n","    accuracy                      0.8574168798      1564\n","   macro avg  0.8573058552 0.8568560012 0.8569256390      1564\n","weighted avg  0.8581977794 0.8574168798 0.8576500592      1564\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0e7rgdNl_b_V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628065306285,"user_tz":-270,"elapsed":21,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}},"outputId":"9bee095e-660c-4f54-de00-24b3fc519390"},"source":["for premise, hypothesis, true_label, predicted_label in evaluation_output[:25]:\n","  print('{}\\t{}\\t{}\\t{}'.format(premise, hypothesis, true_label, predicted_label))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["دوران امامت امام صادق علیه السلام، مصادف است با ترجمه آثار یونانی و گسترش مبارزات فکری و ایدئولوژیکی و نیز ظهور مذاهب و مکتب های انحرافی.\tامام سجاد (ع) در دورانی امامت کردند که همزمان با ترجمه آثار یونانی، ظهور مذاهب و مکتب های انحرافی بود.\tc\tc\n","دوران امامت امام صادق علیه السلام، مصادف است با ترجمه آثار یونانی و گسترش مبارزات فکری و ایدئولوژیکی و نیز ظهور مذاهب و مکتب های انحرافی.\tدستگاه فاسد حکومتی با صرف هزینه های هنگفت، سعی در جعل احادیث و ایجاد انحراف در مکتب تشیع کرده است.\tn\tn\n","با شهادت امام رضا(ع) مرحله جدیدی از تلاش ائمه آغاز شد که «دوران محنت اهل بیت» نام دارد.\tدوران محنت اهل بیت پس از شهادت امام رضا(ع) آغاز گردید.\te\te\n","با شهادت امام رضا(ع) مرحله جدیدی از تلاش ائمه آغاز شد که «دوران محنت اهل بیت» نام دارد.\tبعد از به شهادت رسیدن امام هادی(ع) دوران محنت اهل بیت شروع شد.\tc\tc\n","با شهادت امام رضا(ع) مرحله جدیدی از تلاش ائمه آغاز شد که «دوران محنت اهل بیت» نام دارد.\tحضرت جواد(ع) در سال ۱۹۵ هجری در مدینه ولادت یافت.\tn\tn\n","توحید اَفعالی، باور به اینکه اراده واحدی بر جهان حاکم است. یعنی جز یک اراده، اراده دیگری در تدبیر جهان شرکت ندارد و آن اراده خداوند است.\tاعتقاد به اینکه اراده واحدی بر جهان حاکم است و جز یک اراده در تدبیر جهان شرکت ندارد، بیانگر توحید افعالی است.\te\tc\n","توحید اَفعالی، باور به اینکه اراده واحدی بر جهان حاکم است. یعنی جز یک اراده، اراده دیگری در تدبیر جهان شرکت ندارد و آن اراده خداوند است.\tتوحید ذاتی بیان کننده این است که اراده واحدی بر جهان حاکم است و جز یک اراده در تدبیر جهان اراده دیگری شریک نیست.\tc\tc\n","توحید اَفعالی، باور به اینکه اراده واحدی بر جهان حاکم است. یعنی جز یک اراده، اراده دیگری در تدبیر جهان شرکت ندارد و آن اراده خداوند است.\tتوحید افعالی برخلاف توحید عبادی مانند توحید ذاتی و صفاتی از مراتب نظری توحید است؛ یعنی به حوزه باورهای انسان تعلق دارد.\tn\tn\n","در آیین اسلام، تقیه، به معنای به کار بردن سپر برای هر چه بیشتر ضربه زدن و هر چه کمتر ضربه خوردن است و مفهوم آن، پنهان کردن مذهب خویش در مواردی است که ضرر جانی یا مالی متوجه شخص باشد.\tتقیه در فرهنگ اسلامی به معنای بیشتر ضربه زدن و کمتر ضربه خوردن در پناه سپر است.\te\te\n","در آیین اسلام، تقیه، به معنای به کار بردن سپر برای هر چه بیشتر ضربه زدن و هر چه کمتر ضربه خوردن است و مفهوم آن، پنهان کردن مذهب خویش در مواردی است که ضرر جانی یا مالی متوجه شخص باشد.\tکنار گذاشتن مبارزات سیاسی برای مدت کوتاه را در صورت جلوگیری از ضرر جانی یا مالی، تقیه می‌گویند.\tc\tc\n","در آیین اسلام، تقیه، به معنای به کار بردن سپر برای هر چه بیشتر ضربه زدن و هر چه کمتر ضربه خوردن است و مفهوم آن، پنهان کردن مذهب خویش در مواردی است که ضرر جانی یا مالی متوجه شخص باشد.\tشباهت تقیه و نفاق این است که شخص در هر دو، چیزی را که به آن معتقد است، پنهان می‌کند.\tn\tc\n","خزان عشق یا شد خزان از جمله ترانه‌های مشهور و ماندگار فارسی با متنی عاشقانه از رهی معیری است که نخستین بار در سال ۱۳۱۳ با آهنگسازی و صدای جواد بدیع‌زاده اجرا شد و از معدود ترانه‌های خاطره‌انگیزی است که با گذشت بیش از ۸۰ سال تاکنون جاذبهٔ خود را در میان مردم حفظ کرده‌است. \tترانه \"شد خزان\" برای اولین بار توسط جواد بدیع‌زاده اجرا شد.\te\te\n","خزان عشق یا شد خزان از جمله ترانه‌های مشهور و ماندگار فارسی با متنی عاشقانه از رهی معیری است که نخستین بار در سال ۱۳۱۳ با آهنگسازی و صدای جواد بدیع‌زاده اجرا شد و از معدود ترانه‌های خاطره‌انگیزی است که با گذشت بیش از ۸۰ سال تاکنون جاذبهٔ خود را در میان مردم حفظ کرده‌است. \tبرای نخستین بار، هوشمند عقیلی‌ ترانه \"شد خزان\" را اجرا کرد.\tc\tc\n","خزان عشق یا شد خزان از جمله ترانه‌های مشهور و ماندگار فارسی با متنی عاشقانه از رهی معیری است که نخستین بار در سال ۱۳۱۳ با آهنگسازی و صدای جواد بدیع‌زاده اجرا شد و از معدود ترانه‌های خاطره‌انگیزی است که با گذشت بیش از ۸۰ سال تاکنون جاذبهٔ خود را در میان مردم حفظ کرده‌است. \tاشعار رهی تحت تأثیر سعدی،حافظ، نظامی، صائب و مولوی است.\tn\tn\n","مسجد جامع بصره یک نمونه از مساجد آموزشی است که مهمترین تأثیر علمی - آموزشی این مسجد، راه اندازی جریان فکری - سیاسی معتزله است.\tرشد علم صرف و نحو و قواعد آن، اصلی‌ترین تأثیر مسجد جامع بصره در زمینه علمی - آموزشی بود.\tc\tc\n","مسجد جامع بصره یک نمونه از مساجد آموزشی است که مهمترین تأثیر علمی - آموزشی این مسجد، راه اندازی جریان فکری - سیاسی معتزله است.\tمسجد جامع بصره، مسجد جامع کوفه و مسجد جامع فسطاط از مهمترین نمونه‌های مساجد آموزشی هستند.\tn\tc\n","حقوق جنگ و صلح کتابی به زبان لاتین است که در سال ۱۶۲۵ میلادی توسط حقوقدانی اهل هلند به نام هوگو گروتیوس (گروسیوس) به رشته تحریر درآمد. در این کتاب برای نخستین بار مقررات و قواعد حقوق بین‌الملل بیان شد.\tکتاب قانون جنگ و صلح به عنوان نخستین نوشته در خصوص حقوق بین الملل توسط هوگو گروتیوس هلندی نوشته شد.\te\te\n","حقوق جنگ و صلح کتابی به زبان لاتین است که در سال ۱۶۲۵ میلادی توسط حقوقدانی اهل هلند به نام هوگو گروتیوس (گروسیوس) به رشته تحریر درآمد. در این کتاب برای نخستین بار مقررات و قواعد حقوق بین‌الملل بیان شد.\tجان لاک هلندی برای اولین بار کتابی درباره حقوق بین الملل را تحت عنوان قانون جنگ و صلح نوشته است.\tc\tc\n","حقوق جنگ و صلح کتابی به زبان لاتین است که در سال ۱۶۲۵ میلادی توسط حقوقدانی اهل هلند به نام هوگو گروتیوس (گروسیوس) به رشته تحریر درآمد. در این کتاب برای نخستین بار مقررات و قواعد حقوق بین‌الملل بیان شد.\tگروتیوس که به پدر قوانین بین المللی شهرت یافته با بکارگیری نیروی مسلح برای اجرای سیاست ها مخالف است.\tn\tn\n","نکته مهم و جالب توجه این است که هر وقت دولت انگلستان به فکر تجزیه خوزستان می‌افتاد، دولت عراق را به عنوان مجری و ابزار مورد اعتماد نامزد می‌کرد، این امر به یقین، سرسپردگی و حلقه به‌گوشی سردمداران رژیم عراق نسبت به دولت انگلیس را به اثبات می‌رساند. \tهر وقت دولت انگلیس به فکر تجزیه خوزستان می‌‌افتاد عراق را به عنوان مجری مورد اعتماد خود نامزد می‌کرد.\te\te\n","نکته مهم و جالب توجه این است که هر وقت دولت انگلستان به فکر تجزیه خوزستان می‌افتاد، دولت عراق را به عنوان مجری و ابزار مورد اعتماد نامزد می‌کرد، این امر به یقین، سرسپردگی و حلقه به‌گوشی سردمداران رژیم عراق نسبت به دولت انگلیس را به اثبات می‌رساند. \tدولت انگلیس، ایلات و عشایر جنوب را ابزار مناسبی برای تجزیه خوزستان می‌دانست و از آنها برای نیل به اهداف خود استفاده می‌کرد.\tc\tc\n","نکته مهم و جالب توجه این است که هر وقت دولت انگلستان به فکر تجزیه خوزستان می‌افتاد، دولت عراق را به عنوان مجری و ابزار مورد اعتماد نامزد می‌کرد، این امر به یقین، سرسپردگی و حلقه به‌گوشی سردمداران رژیم عراق نسبت به دولت انگلیس را به اثبات می‌رساند. \tاستان خوزستان به دلیل برخورداری از ذخایر سرشار نفتی، همواره مورد توجه و طمع جهانخواران بوده است و خواهد بود.\tn\tn\n","انگیزه های حضور گسترده ی مردم در جبهه عبارتند از: الف) احساس تکلیف برای دفاع؛ ب) دفاع ازموجودیت اسلام و انقلاب اسلامی و حفظ تمامیت ارضی کشور .\tانگیزه حضور گسترده مردم در جبهه ، دفاع از موجودیت اسلام و انقلاب اسلامی و حفظ تمامیت ارضی کشور بود. \te\te\n","انگیزه های حضور گسترده ی مردم در جبهه عبارتند از: الف) احساس تکلیف برای دفاع؛ ب) دفاع ازموجودیت اسلام و انقلاب اسلامی و حفظ تمامیت ارضی کشور .\tشکوفایی استعدادها، یکی از انگیزه‌های حضور گسترده مردم در جبهه بود.\tc\tc\n","انگیزه های حضور گسترده ی مردم در جبهه عبارتند از: الف) احساس تکلیف برای دفاع؛ ب) دفاع ازموجودیت اسلام و انقلاب اسلامی و حفظ تمامیت ارضی کشور .\tاستکبار و ایادی‌اش با تحمیل جنگ، نابودی اسلام و موجودیت انقلاب اسلامی را هدف قرار داده بودند. \tn\tc\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4MMzwfWLXLWM","executionInfo":{"status":"ok","timestamp":1628065307853,"user_tz":-270,"elapsed":1585,"user":{"displayName":"Zohreh Fallah","photoUrl":"","userId":"16168249174167062064"}}},"source":["output_file_name = \"textual_entailment-parsinlu-farstail_{}_outputs.txt\".format(model_name.replace('/','-'))\n","with open(output_file_name, \"w\", encoding='utf8') as output_file:\n","  for premise, hypothesis, true_label, predicted_label in evaluation_output:\n","    output_file.write('{}\\t{}\\t{}\\t{}\\n'.format(premise, hypothesis, true_label, predicted_label))\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","upload = drive.CreateFile({'title': output_file_name})\n","upload.SetContentFile(output_file_name)\n","upload.Upload()"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNVAUxNPx66s"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BBOVuJYmAwoe"},"source":[""],"execution_count":null,"outputs":[]}]}