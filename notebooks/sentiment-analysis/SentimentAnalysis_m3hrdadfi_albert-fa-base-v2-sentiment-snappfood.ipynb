{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SentimentAnalysis/m3hrdadfi/albert-fa-base-v2-sentiment-snappfood.ipynb","provenance":[{"file_id":"1bSM_SjQpsbgYSOctlP6pvolT4_SckeID","timestamp":1627916795185}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b7d09fb1ffb14756a38524f6f152e719":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0de9b5557f8a4be194018cbbce07c0ee","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c0b672d3a21847b7b48b5daf6a9c918b","IPY_MODEL_3cd42bab9469456f9b26b80777920de8"]}},"0de9b5557f8a4be194018cbbce07c0ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c0b672d3a21847b7b48b5daf6a9c918b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_51f1e359615849e7b9f26e4344d49ec4","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":836,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":836,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d7ef20d1e1d74dfaaf78c3ee89d3d36f"}},"3cd42bab9469456f9b26b80777920de8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8cd3a74908fc4553b95d01b05803127a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 836/836 [00:02&lt;00:00, 280B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5dcc5fe8c2e94f44a1499b2b01b3e4d2"}},"51f1e359615849e7b9f26e4344d49ec4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d7ef20d1e1d74dfaaf78c3ee89d3d36f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8cd3a74908fc4553b95d01b05803127a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5dcc5fe8c2e94f44a1499b2b01b3e4d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"07e55469b1114e8e941f2e349659a20c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ede453674dc04425b3787d52598259be","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_172b9368d43b4792acec3132abbfe78e","IPY_MODEL_ea48b93d7f2441dab72ba17b56af7f4f"]}},"ede453674dc04425b3787d52598259be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"172b9368d43b4792acec3132abbfe78e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_31cb9460fba54b1d80e201dd5609e9f5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1882978,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1882978,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dc5b43c93eba4ed8a252618af92180b4"}},"ea48b93d7f2441dab72ba17b56af7f4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9f721f193f634d1cb93fe7a175925e31","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.88M/1.88M [00:01&lt;00:00, 1.21MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d615408d10247179e0ee26b5bdcfe86"}},"31cb9460fba54b1d80e201dd5609e9f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dc5b43c93eba4ed8a252618af92180b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9f721f193f634d1cb93fe7a175925e31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8d615408d10247179e0ee26b5bdcfe86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aaac0c71d5a842daa93b8821d18b6dee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_613066dd61994b1a85509887b85b33c0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_27a5ef1e4ab84645b9c41d3f9266cdb2","IPY_MODEL_fc750bbd85cc457a8015ce28da74252a"]}},"613066dd61994b1a85509887b85b33c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"27a5ef1e4ab84645b9c41d3f9266cdb2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8aec6eaaaa1d49ec82ba0d60efaffd29","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":156,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":156,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_af5c57583918461a85cf3dd646a4f29e"}},"fc750bbd85cc457a8015ce28da74252a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_94f0acd4c82f4ee18fd7aa88ef1a8ac2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 156/156 [00:00&lt;00:00, 192B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2d756256921c41e79de28c4e98eb695c"}},"8aec6eaaaa1d49ec82ba0d60efaffd29":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"af5c57583918461a85cf3dd646a4f29e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"94f0acd4c82f4ee18fd7aa88ef1a8ac2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2d756256921c41e79de28c4e98eb695c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d3823476f2774af98088477a6e0e054f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b21e1fce95d14f6d88a1ccf7df43cf86","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_16d72a326a704bbb8d848d900f0b6fe7","IPY_MODEL_256083e1608b404b9c1cb2466a0e5630"]}},"b21e1fce95d14f6d88a1ccf7df43cf86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"16d72a326a704bbb8d848d900f0b6fe7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b9c78f1b1337452ea2b7588c9a221436","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":62,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":62,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8fdada67461a48bfa8058e5e40bf4f44"}},"256083e1608b404b9c1cb2466a0e5630":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6c28545b297547a38816489f5f442585","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 62.0/62.0 [00:00&lt;00:00, 289B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_84d263506aff4e458ad96113e6dd7832"}},"b9c78f1b1337452ea2b7588c9a221436":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8fdada67461a48bfa8058e5e40bf4f44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c28545b297547a38816489f5f442585":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"84d263506aff4e458ad96113e6dd7832":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a96c05b358b64348a47b93569375336e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_aa04f622915346829c2c88b03cdb4a23","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a6c5b1ee44be47b389af433f379e1198","IPY_MODEL_57cda7a2721544cb977fdb271afa4087"]}},"aa04f622915346829c2c88b03cdb4a23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a6c5b1ee44be47b389af433f379e1198":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6530959fad8f48e59c10601621ff1768","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":72346844,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":72346844,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7c3ea0a123bb4d7bb58964250e4aee17"}},"57cda7a2721544cb977fdb271afa4087":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4429ca2bc7fa46c8935f7f5282f9dbd5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 72.3M/72.3M [00:17&lt;00:00, 4.03MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d14186b957cf4fd98cad5c7274877b13"}},"6530959fad8f48e59c10601621ff1768":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7c3ea0a123bb4d7bb58964250e4aee17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4429ca2bc7fa46c8935f7f5282f9dbd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d14186b957cf4fd98cad5c7274877b13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"EVTLnae70XUx"},"source":["# Persian Sentiment\n","It aims to classify text, such as comments, based on their emotional bias. We tested three well-known datasets for this task: **Digikala** user comments, **SnappFood** user comments, and **DeepSentiPers** in two binary-form and multi-form types.\n"]},{"cell_type":"code","metadata":{"id":"o_zHi1zlwyXb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627917946265,"user_tz":-270,"elapsed":583,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"f4df1b73-045b-46dd-939c-6d605f10673d"},"source":["!nvidia-smi\n","!lscpu"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mon Aug  2 15:25:45 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Architecture:        x86_64\n","CPU op-mode(s):      32-bit, 64-bit\n","Byte Order:          Little Endian\n","CPU(s):              2\n","On-line CPU(s) list: 0,1\n","Thread(s) per core:  2\n","Core(s) per socket:  1\n","Socket(s):           1\n","NUMA node(s):        1\n","Vendor ID:           GenuineIntel\n","CPU family:          6\n","Model:               79\n","Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n","Stepping:            0\n","CPU MHz:             2199.998\n","BogoMIPS:            4399.99\n","Hypervisor vendor:   KVM\n","Virtualization type: full\n","L1d cache:           32K\n","L1i cache:           32K\n","L2 cache:            256K\n","L3 cache:            56320K\n","NUMA node0 CPU(s):   0,1\n","Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4knw0YgC0SfX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627917974974,"user_tz":-270,"elapsed":28713,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"031ab170-5ddd-4d68-f398-09498af5f4d0"},"source":["!pip install hazm==0.7.0\n","!pip install seqeval==1.2.2\n","!pip install sentencepiece==0.1.96\n","!pip install transformers==4.7.0\n","!pip install clean-text[gpl]==0.4.0"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting hazm==0.7.0\n","  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n","\u001b[?25l\r\u001b[K     |█                               | 10 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 51 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 61 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 71 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 81 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 92 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 102 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 112 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 122 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 133 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 143 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 153 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 163 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 174 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 184 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 194 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 204 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 215 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 225 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 235 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 245 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 256 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 266 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 276 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 286 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 296 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 307 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 316 kB 7.8 MB/s \n","\u001b[?25hCollecting libwapiti>=0.2.1\n","  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n","\u001b[K     |████████████████████████████████| 233 kB 13.1 MB/s \n","\u001b[?25hCollecting nltk==3.3\n","  Downloading nltk-3.3.0.zip (1.4 MB)\n","\u001b[K     |████████████████████████████████| 1.4 MB 15.2 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm==0.7.0) (1.15.0)\n","Building wheels for collected packages: nltk, libwapiti\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394487 sha256=c0b0c50a83114c8bd63f0bf691b5d7ef076defd381af0bd29ea04517bb751f04\n","  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n","  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154547 sha256=5add9b7ce9604a99986c13d3a2507f859cabcf9fd8673bbfd684455c6578f23a\n","  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n","Successfully built nltk libwapiti\n","Installing collected packages: nltk, libwapiti, hazm\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n","Collecting seqeval==1.2.2\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval==1.2.2) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval==1.2.2) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.0.1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval==1.2.2) (1.4.1)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=0af3f627c1eb66f156fb48f833886b442e1658bb4deb12965a401d06690140e5\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n","Collecting sentencepiece==0.1.96\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 8.0 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Collecting transformers==4.7.0\n","  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 7.8 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (4.6.1)\n","Collecting huggingface-hub==0.0.8\n","  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 61.9 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (3.13)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (2019.12.20)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (21.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 56.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.7.0) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.7.0) (3.5.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0) (1.0.1)\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.7.0\n","Collecting clean-text[gpl]==0.4.0\n","  Downloading clean_text-0.4.0-py3-none-any.whl (9.8 kB)\n","Collecting emoji\n","  Downloading emoji-1.4.2.tar.gz (184 kB)\n","\u001b[K     |████████████████████████████████| 184 kB 8.9 MB/s \n","\u001b[?25hCollecting ftfy<7.0,>=6.0\n","  Downloading ftfy-6.0.3.tar.gz (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 3.4 MB/s \n","\u001b[?25hCollecting unidecode<2.0.0,>=1.1.1\n","  Downloading Unidecode-1.2.0-py2.py3-none-any.whl (241 kB)\n","\u001b[K     |████████████████████████████████| 241 kB 30.4 MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text[gpl]==0.4.0) (0.2.5)\n","Building wheels for collected packages: ftfy, emoji\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41934 sha256=6c895aa83a7505c2d1bdcff9d46bdca6e88bd34b8e18fb982135919553abfb81\n","  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186469 sha256=8d421c333e23925fc2afe49286d2844ff5afc5bd407be11ae18f69c3e86036fe\n","  Stored in directory: /root/.cache/pip/wheels/e4/61/e7/2fc1ac8f306848fc66c6c013ab511f0a39ef4b1825b11363b2\n","Successfully built ftfy emoji\n","Installing collected packages: ftfy, emoji, unidecode, clean-text\n","Successfully installed clean-text-0.4.0 emoji-1.4.2 ftfy-6.0.3 unidecode-1.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lg-I-r-32ep6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627918005479,"user_tz":-270,"elapsed":30512,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"e3810f89-e737-474c-e87b-19d87bea1940"},"source":["!pip install PyDrive\n","import os\n","import IPython.display as ipd\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: PyDrive in /usr/local/lib/python3.7/dist-packages (1.3.1)\n","Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (1.12.8)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (4.1.3)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (3.13)\n","Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.26.3)\n","Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.32.1)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n","Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n","Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.23.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2018.9)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (57.2.0)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (21.0)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.17.3)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.53.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.2->PyDrive) (0.2.8)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.24.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HjQo6WGZ2aK5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627918014541,"user_tz":-270,"elapsed":6418,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"23b8f1ad-3fad-4fc7-d3c9-337178ee3012"},"source":["# Import required packages\n","import os\n","import gc\n","import re\n","import hazm\n","import time\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import transformers\n","from transformers import AutoConfig, AutoTokenizer\n","from transformers import AutoModelForSequenceClassification\n","\n","from cleantext import clean\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","print()\n","print('numpy', np.__version__)\n","print('pandas', pd.__version__)\n","print('transformers', transformers.__version__)\n","print('torch', torch.__version__)\n","print()\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["\n","numpy 1.19.5\n","pandas 1.1.5\n","transformers 4.7.0\n","torch 1.9.0+cu102\n","\n","There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5vC31D-N0Shj","executionInfo":{"status":"ok","timestamp":1627918017266,"user_tz":-270,"elapsed":2729,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}}},"source":["class SentimentAnalysisDataset(torch.utils.data.Dataset):\n","    \"\"\" Create a PyTorch dataset for Sentiment Analysis. \"\"\"\n","\n","    def __init__(self, tokenizer, comments, targets, label_list=None, max_len=128):\n","        self.comments = comments\n","        self.targets = targets\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.label2index = {label: i for i, label in enumerate(label_list)} if isinstance(label_list, list) else {}\n","        self.index2label = {i: label for label, i in self.label2index.items()}\n","\n","    def __len__(self):\n","        return len(self.comments)\n","\n","    def __getitem__(self, item):\n","        comment = self.comments[item]\n","        target = self.label2index[self.targets[item]]\n","        encoding = self.tokenizer.encode_plus(\n","            comment,\n","            add_special_tokens=True,\n","            truncation=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_tensors='pt')\n","\n","        inputs = {\n","            'comment': comment,\n","            'targets': torch.tensor(target, dtype=torch.long),\n","            'original_targets': self.targets[item],\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'token_type_ids': encoding['token_type_ids'].flatten(),\n","        }\n","\n","        return inputs\n","\n","\n","class MT5SentimentAnalysisDataset(torch.utils.data.Dataset):\n","    \"\"\" Create a PyTorch dataset for Sentiment Analysis. \"\"\"\n","\n","    def __init__(self, reviews, aspects, labels, tokenizer, max_length=128):\n","        self.reviews = reviews\n","        self.aspects = aspects\n","        self.targets = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.reviews)\n","\n","    def __getitem__(self, item):\n","        if self.aspects is not None:\n","            encoding = self.tokenizer(\n","                self.reviews[item] + \" <sep> \" + self.aspects[item],\n","                add_special_tokens=True,\n","                max_length=self.max_length,\n","                truncation=True,\n","                padding='max_length',\n","                return_tensors=\"pt\"\n","            )\n","            inputs = {\n","                'review': self.reviews[item],\n","                'aspects': self.aspects[item],\n","                'targets': self.targets[item],\n","                'input_ids': encoding['input_ids'].flatten(),\n","                'attention_mask': encoding['attention_mask'].flatten()\n","            }\n","        else:\n","            encoding = self.tokenizer(\n","                self.reviews[item],\n","                add_special_tokens=True,\n","                max_length=self.max_length,\n","                truncation=True,\n","                padding='max_length',\n","                return_tensors=\"pt\"\n","            )\n","            inputs = {\n","                'review': self.reviews[item],\n","                'targets': self.targets[item],\n","                'input_ids': encoding['input_ids'].flatten(),\n","                'attention_mask': encoding['attention_mask'].flatten()\n","            }\n","\n","        return inputs\n","\n","\n","class SentimentAnalysis:\n","    def __init__(self, model_name, model_type=None):\n","        self.normalizer = hazm.Normalizer()\n","        self.model_name = model_name\n","        if model_type == \"mt5\":\n","            self.tokenizer = MT5Tokenizer.from_pretrained(model_name)\n","            self.model = MT5ForConditionalGeneration.from_pretrained(model_name)\n","            self.config = MT5Config.from_pretrained(self.model_name)\n","        else:\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n","            self.config = AutoConfig.from_pretrained(self.model_name)\n","            self.id2label = self.config.id2label\n","            self.label2id = self.config.label2id\n","\n","    def cleaning(self, text):\n","        def cleanhtml(raw_html):\n","            clean_pattern = re.compile('<.*?>')\n","            clean_text = re.sub(clean_pattern, '', raw_html)\n","            return clean_text\n","\n","        if type(text) is not str:\n","            return None\n","\n","        text = text.strip()\n","\n","        # regular cleaning\n","        text = clean(\n","            text,\n","            fix_unicode=True,\n","            to_ascii=False,\n","            lower=True,\n","            no_line_breaks=True,\n","            no_urls=True,\n","            no_emails=True,\n","            no_phone_numbers=True,\n","            no_numbers=False,\n","            no_digits=False,\n","            no_currency_symbols=True,\n","            no_punct=False,\n","            replace_with_url=\"\",\n","            replace_with_email=\"\",\n","            replace_with_phone_number=\"\",\n","            replace_with_number=\"\",\n","            replace_with_digit=\"0\",\n","            replace_with_currency_symbol=\"\"\n","        )\n","\n","        # cleaning htmls\n","        text = cleanhtml(text)\n","\n","        # normalizing\n","        text = self.normalizer.normalize(text)\n","\n","        # removing wierd patterns\n","        wierd_pattern = re.compile(\"[\"\n","                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                                   u\"\\U00002702-\\U000027B0\"\n","                                   u\"\\U000024C2-\\U0001F251\"\n","                                   u\"\\U0001f926-\\U0001f937\"\n","                                   u'\\U00010000-\\U0010ffff'\n","                                   u\"\\u200d\"\n","                                   u\"\\u2640-\\u2642\"\n","                                   u\"\\u2600-\\u2B55\"\n","                                   u\"\\u23cf\"\n","                                   u\"\\u23e9\"\n","                                   u\"\\u231a\"\n","                                   u\"\\u3030\"\n","                                   u\"\\ufe0f\"\n","                                   u\"\\u2069\"\n","                                   u\"\\u2066\"\n","                                   # u\"\\u200c\"\n","                                   u\"\\u2068\"\n","                                   u\"\\u2067\"\n","                                   \"]+\", flags=re.UNICODE)\n","\n","        text = wierd_pattern.sub(r'', text)\n","\n","        # removing extra spaces, hashtags\n","        text = re.sub(\"#\", \"\", text)\n","        text = re.sub(\"\\s+\", \" \", text)\n","        if text in ['', \" \"]:\n","            return None\n","        return text\n","\n","    def load_dataset_test_file(self, dataset_name, dataset_file, **kwargs):\n","        if dataset_name.lower() == \"snappfood\":\n","            if not os.path.exists(dataset_file):\n","                print(f'{dataset_file} not exists!')\n","                return\n","            data = pd.read_csv(dataset_file, delimiter=\"\\t\")\n","            # drop label_id because its not consistent with albert model labels!\n","            data = data[['comment', 'label']]\n","\n","            # cleaning comments\n","            data = data.dropna(subset=['comment'])\n","            data['comment'] = data['comment'].apply(self.cleaning)\n","            data = data.dropna(subset=['comment'])\n","\n","            if 'label_map' in kwargs:\n","                data['label'] = data['label'].apply(lambda l: kwargs['label_map'][l])\n","                data = data.dropna(subset=['label'])\n","                data = data.reset_index(drop=True)\n","\n","            data['label_id'] = data['label'].apply(lambda t: self.label2id[t])\n","            x_test, y_test = data['comment'].values.tolist(), data['label_id'].values.tolist()\n","            print(f'test part:\\n #comment: {len(x_test)}, #labels: {len(y_test)}')\n","            return x_test, y_test\n","        if dataset_name.lower() == \"deepsentipers\":\n","            if not os.path.exists(dataset_file):\n","                print(f'{dataset_file} not exists!')\n","                return\n","            if 'label_map' not in kwargs:\n","                print(\"label_map is missing!\")\n","                return\n","            data = pd.read_csv(dataset_file, delimiter=\",\", names=['comment', 'label'], header=None)\n","\n","            # cleaning comments\n","            data = data.dropna(subset=['comment'])\n","            data['comment'] = data['comment'].apply(self.cleaning)\n","            data = data.dropna(subset=['comment'])\n","\n","            # map labels\n","            label_map = kwargs['label_map']\n","            data['label'] = data['label'].apply(lambda l: label_map[l])\n","            data = data.dropna(subset=['label'])\n","            data = data.reset_index(drop=True)\n","\n","            data['label_id'] = data['label'].apply(lambda t: self.label2id[t])\n","            x_test, y_test = data['comment'].values.tolist(), data['label_id'].values.tolist()\n","            print(f'test part:\\n #comment: {len(x_test)}, #labels: {len(y_test)}')\n","            return x_test, y_test\n","        if dataset_name.lower() == \"pasinlu-aspect-sentiment\":\n","            if not os.path.exists(dataset_file):\n","                print(f'{dataset_file} not exists!')\n","                return\n","            if 'label_map' not in kwargs:\n","                print(\"label_map is missing!\")\n","                return\n","\n","            reviews, aspects, labels = [], [], []\n","            with open(dataset_file, encoding=\"utf8\") as infile:\n","                for line in infile:\n","                    json_line = json.loads(line.strip())\n","\n","                    review = json_line['review']\n","                    reviews.append(review)\n","\n","                    question = json_line['question']\n","                    aspects.append(question)\n","\n","                    label = kwargs['label_map'][json_line['label']]\n","                    labels.append(label)\n","\n","            return reviews, aspects, labels\n","\n","    def load_dataset_file(self, dataset_name, dataset_file, **kwargs):\n","        if dataset_name.lower() == \"digikala\":\n","            if not os.path.exists(dataset_file):\n","                print(f'{dataset_file} not exists!')\n","                return\n","            data = pd.read_excel(dataset_file)\n","            data = data[['comment', 'recommend']]\n","\n","            # cleaning comments\n","            data = data.dropna(subset=['comment'])\n","            data['comment'] = data['comment'].apply(self.cleaning)\n","            data = data.dropna(subset=['comment'])\n","\n","            # cleaning labels\n","            valid_labels = ['no_idea', 'not_recommended', 'recommended']\n","            data['recommend'] = data['recommend'].apply(lambda r: r if r in valid_labels else None)\n","            data = data.dropna(subset=['recommend'])\n","            if 'label_map' in kwargs:\n","                data['recommend'] = data['recommend'].apply(lambda l: kwargs['label_map'][l])\n","            data = data.dropna(subset=['recommend'])\n","            data = data.reset_index(drop=True)\n","\n","            data['label_id'] = data['recommend'].apply(lambda t: self.label2id[t])\n","\n","            x_all, y_all = data['comment'].values.tolist(), data['label_id'].values.tolist()\n","            print(f'all data: #comment: {len(x_all)}, #labels: {len(y_all)}')\n","\n","            _, test = train_test_split(data, test_size=0.1, random_state=1, stratify=data['recommend'])\n","            test = test.reset_index(drop=True)\n","            x_test, y_test = test['comment'].values.tolist(), test['label_id'].values.tolist()\n","            print(f'test part:\\n #comment: {len(x_test)}, #labels: {len(y_test)}')\n","            return x_all, y_all, x_test, y_test\n","        if dataset_name.lower() == \"pasinlu-review-sentiment\":\n","            if not os.path.exists(dataset_file):\n","                print(f'{dataset_file} not exists!')\n","                return\n","            if 'label_map' not in kwargs:\n","                print(\"label_map is missing!\")\n","                return\n","\n","            reviews, labels = [], []\n","            with open(dataset_file, encoding=\"utf8\") as infile:\n","                for line in infile:\n","                    json_line = json.loads(line.strip())\n","\n","                    review = json_line['review']\n","                    reviews.append(review)\n","\n","                    label = kwargs['label_map'][json_line['sentiment']]\n","                    labels.append(label)\n","            return reviews, labels\n","\n","    def load_dataset_composite_file(self, dataset_name, dataset_files, **kwargs):\n","        if dataset_name.lower() == \"digikala+snappfood+deepsentipers\":\n","            if sorted(list(dataset_files.keys())) != [\"deepsentipers\", \"digikala\", \"snappfood\"]:\n","                print(\"dataset_files must contains path of all three datasets\")\n","                return\n","            if 'label_map' not in kwargs:\n","                print(\"label_map is missing!\")\n","                return\n","            elif sorted(list(kwargs['label_map'].keys())) != [\"deepsentipers\", \"digikala\", \"snappfood\"]:\n","                print(\"label_map must contains label_map for all three datasets!\")\n","                return\n","            print(\"digikala dataset - we only use test set:\")\n","            _, _, x_test_digi, y_test_digi = self.load_dataset_file('digikala', dataset_files['digikala'],\n","                                                                    label_map=kwargs['label_map']['digikala'])\n","            print(\"snappfood dataset:\")\n","            x_test_snapp, y_test_snapp = self.load_dataset_test_file('snappfood', dataset_files['snappfood'],\n","                                                                     label_map=kwargs['label_map']['snappfood'])\n","            print(\"deepsentipers dataset:\")\n","            x_test_senti, y_test_senti = self.load_dataset_test_file('deepsentipers', dataset_files['deepsentipers'],\n","                                                                     label_map=kwargs['label_map']['deepsentipers'])\n","            return x_test_digi + x_test_snapp + x_test_senti, y_test_digi + y_test_snapp + y_test_senti\n","\n","    def sentiment_analysis_inference(self, input_text, device):\n","        if not self.model or not self.tokenizer or not self.id2label:\n","            print('Something wrong has been happened!')\n","            return\n","\n","        pt_batch = self.tokenizer(\n","            input_text,\n","            padding=True,\n","            truncation=True,\n","            max_length=self.config.max_position_embeddings,\n","            return_tensors=\"pt\"\n","        )\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","        pt_batch = pt_batch.to(device)\n","\n","        pt_outputs = self.model(**pt_batch)\n","        pt_predictions = torch.argmax(F.softmax(pt_outputs.logits, dim=1), dim=1)\n","\n","        output_predictions = []\n","        for i, sentence in enumerate(input_text):\n","            output_predictions.append((sentence, self.id2label.get(pt_predictions[i].item())))\n","        return output_predictions\n","\n","    def mt5_sentiment_analysis_inference(self, reviews, device):\n","        if not self.model or not self.tokenizer:\n","            print('Something wrong has been happened!')\n","            return\n","\n","        tokenized_batch = self.tokenizer(\n","            reviews,\n","            padding=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","\n","        input_ids = tokenized_batch.input_ids.to(device)\n","        attention_mask = tokenized_batch.attention_mask.to(device)\n","        outputs = self.model.generate(input_ids=input_ids,\n","                                      attention_mask=attention_mask)\n","        predictions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","        return predictions\n","\n","    def mt5_aspect_sentiment_analysis_inference(self, reviews, aspects, device):\n","        if not self.model or not self.tokenizer:\n","            print('Something wrong has been happened!')\n","            return\n","\n","        new_input = []\n","        for r, a in zip(reviews, aspects):\n","            new_input.append(r + \" <sep> \" + a)\n","\n","        tokenized_batch = self.tokenizer(\n","            new_input,\n","            padding=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","\n","        input_ids = tokenized_batch.input_ids.to(device)\n","        attention_mask = tokenized_batch.attention_mask.to(device)\n","        outputs = self.model.generate(input_ids=input_ids,\n","                                      attention_mask=attention_mask)\n","        predictions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","        return predictions\n","\n","    def evaluation(self, input_text, input_labels, device, batch_size=4):\n","        if not self.model or not self.tokenizer or not self.id2label:\n","            print('Something wrong has been happened!')\n","            return\n","\n","        max_len = self.config.max_position_embeddings\n","        label_list = list(set(input_labels))\n","        label_count = {self.id2label[label]: input_labels.count(label) for label in label_list}\n","        print(\"label_count:\", label_count)\n","        dataset = SentimentAnalysisDataset(comments=input_text, targets=input_labels, tokenizer=self.tokenizer,\n","                                           max_len=max_len, label_list=label_list)\n","        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","\n","        print(\"#samples:\", len(input_text))\n","        print(\"#batch:\", len(data_loader))\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","\n","        total_loss, total_time = 0, 0\n","        output_predictions = []\n","        golden_labels, predicted_labels = [], []\n","        print(\"Start to evaluate test data ...\")\n","        for step, batch in enumerate(data_loader):\n","            b_comments = batch['comment']\n","            b_input_ids = batch['input_ids']\n","            b_attention_mask = batch['attention_mask']\n","            b_token_type_ids = batch['token_type_ids']\n","            b_targets = batch['targets']\n","\n","            # move tensors to GPU if CUDA is available\n","            b_input_ids = b_input_ids.to(device)\n","            b_attention_mask = b_attention_mask.to(device)\n","            b_token_type_ids = b_token_type_ids.to(device)\n","            b_targets = b_targets.to(device)\n","\n","            # This will return the loss (rather than the model output) because we have provided the `labels`.\n","            with torch.no_grad():\n","                start = time.monotonic()\n","                b_outputs = self.model(input_ids=b_input_ids, attention_mask=b_attention_mask,\n","                                       token_type_ids=b_token_type_ids, labels=b_targets)\n","                end = time.monotonic()\n","                total_time += end - start\n","                print(f'inference time for step {step}: {end - start}')\n","            # get the loss\n","            total_loss += b_outputs.loss.item()\n","\n","            b_original_targets = batch['original_targets']\n","            golden_labels.extend(b_original_targets.tolist())\n","\n","            b_predictions = torch.argmax(F.softmax(b_outputs.logits, dim=1), dim=1)\n","            b_predictions = b_predictions.cpu().detach().numpy().tolist()\n","            b_predictions = [dataset.index2label[label] for label in b_predictions]\n","            predicted_labels.extend(b_predictions)\n","\n","            for i, comment in enumerate(b_comments):\n","                output_predictions.append((\n","                    comment,\n","                    self.id2label[b_original_targets[i].item()],\n","                    self.id2label[b_predictions[i]]\n","                ))\n","                # print(f'output prediction: {i},{comment},{self.id2label[b_original_targets[i].item()]},'\n","                #       f'{self.id2label[b_predictions[i]]}')\n","\n","        # Calculate the average loss over the training data.\n","        avg_train_loss = total_loss / len(data_loader)\n","        print(\"average loss:\", avg_train_loss)\n","        print(\"total inference time:\", total_time)\n","        print(\"total inference time / #samples:\", total_time / len(input_text))\n","\n","        # evaluate\n","        print(\"Test Accuracy: {}\".format(accuracy_score(golden_labels, predicted_labels)))\n","        print(\"Test Precision: {}\".format(precision_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test Recall: {}\".format(recall_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test F1-Score(weighted average): {}\".format(\n","            f1_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test classification Report:\\n{}\".format(classification_report(\n","            golden_labels, predicted_labels, digits=10, target_names=[self.id2label[_] for _ in sorted(label_list)])))\n","        return output_predictions\n","\n","    def mt5_sentiment_analysis_evaluation(self, reviews, labels, device, max_length, batch_size=4):\n","        if not self.model or not self.tokenizer:\n","            print('Something wrong has been happened!')\n","            return\n","        if len(reviews) != len(labels):\n","            print('length of inputs and labels is not equal!!')\n","            return\n","\n","        dataset = MT5SentimentAnalysisDataset(reviews=reviews, aspects=None, labels=labels, tokenizer=self.tokenizer,\n","                                              max_length=max_length)\n","        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","        print(f'#reviews:{len(reviews)}, #labels:{len(labels)}')\n","        print(\"#batch:\", len(data_loader))\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","\n","        total_time = 0\n","        output_predictions = []\n","        golden_labels, predicted_labels = [], []\n","        print(\"Start to evaluate test data ...\")\n","        for step, batch in enumerate(data_loader):\n","            # move tensors to GPU if CUDA is available\n","            b_input_ids = batch['input_ids'].to(device)\n","            b_attention_mask = batch['attention_mask'].to(device)\n","\n","            # This will return the loss (rather than the model output) because we have provided the `labels`.\n","            with torch.no_grad():\n","                start = time.monotonic()\n","                b_outputs = self.model.generate(input_ids=b_input_ids, attention_mask=b_attention_mask)\n","                end = time.monotonic()\n","                total_time += end - start\n","                print(f'inference time for step {step}: {end - start}')\n","\n","            b_targets = batch['targets']\n","            golden_labels.extend(b_targets)\n","\n","            b_predictions = self.tokenizer.batch_decode(b_outputs, skip_special_tokens=True)\n","            predicted_labels.extend(b_predictions)\n","\n","            for i, review in enumerate(batch['review']):\n","                output_predictions.append((\n","                    review,\n","                    b_targets[i],\n","                    b_predictions[i]\n","                ))\n","\n","        print(\"total inference time:\", total_time)\n","        print(\"total inference time / #samples:\", total_time / len(reviews))\n","\n","        # evaluate\n","        print(\"Test Accuracy: {}\".format(accuracy_score(golden_labels, predicted_labels)))\n","        print(\"Test Precision: {}\".format(precision_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test Recall: {}\".format(recall_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test F1-Score(weighted average): {}\".format(\n","            f1_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test classification Report:\\n{}\".format(\n","            classification_report(golden_labels, predicted_labels, digits=10)))\n","        return output_predictions\n","\n","    def mt5_aspect_sentiment_analysis_evaluation(self, reviews, aspects, labels, device, max_length, batch_size=4):\n","        if not self.model or not self.tokenizer:\n","            print('Something wrong has been happened!')\n","            return\n","        if len(reviews) != len(labels):\n","            print('length of inputs and labels is not equal!!')\n","            return\n","\n","        dataset = MT5SentimentAnalysisDataset(reviews=reviews, aspects=aspects, labels=labels, tokenizer=self.tokenizer,\n","                                              max_length=max_length)\n","        data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","        print(f'#reviews:{len(reviews)}, #aspects:{len(aspects)}, #labels:{len(labels)}')\n","        print(\"#batch:\", len(data_loader))\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Tell pytorch to run this model on the GPU.\n","        if device.type != 'cpu':\n","            self.model.cuda()\n","        self.model.eval()\n","\n","        total_time = 0\n","        output_predictions = []\n","        golden_labels, predicted_labels = [], []\n","        print(\"Start to evaluate test data ...\")\n","        for step, batch in enumerate(data_loader):\n","            # move tensors to GPU if CUDA is available\n","            b_input_ids = batch['input_ids'].to(device)\n","            b_attention_mask = batch['attention_mask'].to(device)\n","\n","            # This will return the loss (rather than the model output) because we have provided the `labels`.\n","            with torch.no_grad():\n","                start = time.monotonic()\n","                b_outputs = self.model.generate(input_ids=b_input_ids, attention_mask=b_attention_mask)\n","                end = time.monotonic()\n","                total_time += end - start\n","                print(f'inference time for step {step}: {end - start}')\n","\n","            b_targets = batch['targets']\n","            golden_labels.extend(b_targets)\n","\n","            b_predictions = self.tokenizer.batch_decode(b_outputs, skip_special_tokens=True)\n","            predicted_labels.extend(b_predictions)\n","\n","            for i, review in enumerate(batch['review']):\n","                output_predictions.append((\n","                    review,\n","                    batch['aspects'][i],\n","                    b_targets[i],\n","                    b_predictions[i]\n","                ))\n","\n","        print(\"total inference time:\", total_time)\n","        print(\"total inference time / #samples:\", total_time / len(reviews))\n","\n","        # evaluate\n","        print(\"Test Accuracy: {}\".format(accuracy_score(golden_labels, predicted_labels)))\n","        print(\"Test Precision: {}\".format(precision_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test Recall: {}\".format(recall_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test F1-Score(weighted average): {}\".format(\n","            f1_score(golden_labels, predicted_labels, average=\"weighted\")))\n","        print(\"Test classification Report:\\n{}\".format(\n","            classification_report(golden_labels, predicted_labels, digits=10)))\n","        return output_predictions\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"VD0FH_FF2oTy","colab":{"base_uri":"https://localhost:8080/","height":950,"referenced_widgets":["b7d09fb1ffb14756a38524f6f152e719","0de9b5557f8a4be194018cbbce07c0ee","c0b672d3a21847b7b48b5daf6a9c918b","3cd42bab9469456f9b26b80777920de8","51f1e359615849e7b9f26e4344d49ec4","d7ef20d1e1d74dfaaf78c3ee89d3d36f","8cd3a74908fc4553b95d01b05803127a","5dcc5fe8c2e94f44a1499b2b01b3e4d2","07e55469b1114e8e941f2e349659a20c","ede453674dc04425b3787d52598259be","172b9368d43b4792acec3132abbfe78e","ea48b93d7f2441dab72ba17b56af7f4f","31cb9460fba54b1d80e201dd5609e9f5","dc5b43c93eba4ed8a252618af92180b4","9f721f193f634d1cb93fe7a175925e31","8d615408d10247179e0ee26b5bdcfe86","aaac0c71d5a842daa93b8821d18b6dee","613066dd61994b1a85509887b85b33c0","27a5ef1e4ab84645b9c41d3f9266cdb2","fc750bbd85cc457a8015ce28da74252a","8aec6eaaaa1d49ec82ba0d60efaffd29","af5c57583918461a85cf3dd646a4f29e","94f0acd4c82f4ee18fd7aa88ef1a8ac2","2d756256921c41e79de28c4e98eb695c","d3823476f2774af98088477a6e0e054f","b21e1fce95d14f6d88a1ccf7df43cf86","16d72a326a704bbb8d848d900f0b6fe7","256083e1608b404b9c1cb2466a0e5630","b9c78f1b1337452ea2b7588c9a221436","8fdada67461a48bfa8058e5e40bf4f44","6c28545b297547a38816489f5f442585","84d263506aff4e458ad96113e6dd7832","a96c05b358b64348a47b93569375336e","aa04f622915346829c2c88b03cdb4a23","a6c5b1ee44be47b389af433f379e1198","57cda7a2721544cb977fdb271afa4087","6530959fad8f48e59c10601621ff1768","7c3ea0a123bb4d7bb58964250e4aee17","4429ca2bc7fa46c8935f7f5282f9dbd5","d14186b957cf4fd98cad5c7274877b13"]},"executionInfo":{"status":"ok","timestamp":1627918025263,"user_tz":-270,"elapsed":8003,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"94f81b3d-94c3-43c7-cda4-c8fb2c74d8fb"},"source":["model_name='m3hrdadfi/albert-fa-base-v2-sentiment-snappfood'\n","sa_model = SentimentAnalysis(model_name)\n","print(sa_model.config)"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7d09fb1ffb14756a38524f6f152e719","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=836.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07e55469b1114e8e941f2e349659a20c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1882978.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aaac0c71d5a842daa93b8821d18b6dee","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=156.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3823476f2774af98088477a6e0e054f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=62.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a96c05b358b64348a47b93569375336e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=72346844.0, style=ProgressStyle(descrip…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","AlbertConfig {\n","  \"architectures\": [\n","    \"AlbertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"finetuning_task\": \"snappfood\",\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"SAD\",\n","    \"1\": \"HAPPY\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"HAPPY\": 1,\n","    \"SAD\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.7.0\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 80000\n","}\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Yv8v5YOv21Qe"},"source":["## Sample Inference:"]},{"cell_type":"code","metadata":{"id":"XrvVQdG_218P","executionInfo":{"status":"ok","timestamp":1627918039942,"user_tz":-270,"elapsed":593,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}}},"source":["texts = [\n","    \"خوب نبود اصلا\",\n","    \"از رنگش خوشم نیومد\",\n","    \"کیفیتیش عالی بود\"\n","]"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZMeNJ2k23N2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627918052353,"user_tz":-270,"elapsed":12416,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"a75e3dfe-93c3-4a53-8b21-5593f06380f2"},"source":["sa_model.sentiment_analysis_inference(texts, device)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('خوب نبود اصلا', 'SAD'),\n"," ('از رنگش خوشم نیومد', 'HAPPY'),\n"," ('کیفیتیش عالی بود', 'HAPPY')]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"XEWiWbu621Tk"},"source":["## SnappFood\n","\n","Snappfood (an online food delivery company) user comments containing 70,000 comments with two labels (i.e. polarity classification):\n","1. Happy\n","2. Sad\n","\n","Dataset statistics:\n","\n","|          Label         | # | \n","|:------------------------:|:-----------:|\n","|  Negative  |      35000    |\n","|  Positive |      35000      |\n","\n","Test set statistics:\n","\n","|          Label         | # | \n","|:------------------------:|:-----------:|\n","|  Negative  |      3500    |\n","|  Positive |      3500      |\n","\t\t\n","Download You can download the dataset from [here](https://drive.google.com/uc?id=15J4zPN1BD7Q_ZIQ39VeFquwSoW8qTxgu)"]},{"cell_type":"code","metadata":{"id":"YGTOdeQh_Hrt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627918055477,"user_tz":-270,"elapsed":3131,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"d5d172c0-8912-43a9-be75-c602e5e68e59"},"source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","download = drive.CreateFile({'id': '15J4zPN1BD7Q_ZIQ39VeFquwSoW8qTxgu'})\n","download.GetContentFile('snappfood.zip')\n","!ls"],"execution_count":9,"outputs":[{"output_type":"stream","text":["adc.json  sample_data  snappfood.zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AA_0TV3z0LJx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627918056190,"user_tz":-270,"elapsed":723,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"e2d74f70-e5d1-4b02-8875-eaf7c8bb2bf7"},"source":["!unzip snappfood.zip\n","!ls\n","!ls snappfood"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Archive:  snappfood.zip\n","   creating: snappfood/\n","  inflating: snappfood/dev.csv       \n","  inflating: snappfood/train.csv     \n","  inflating: snappfood/test.csv      \n","adc.json  sample_data  snappfood  snappfood.zip\n","dev.csv  test.csv  train.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SookPMv752pv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627918058467,"user_tz":-270,"elapsed":2282,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"c3f8de9a-9ed0-47e6-f500-facab6ef9994"},"source":["test_comments, test_labels = sa_model.load_dataset_test_file(dataset_name=\"snappfood\", dataset_file=\"./snappfood/test.csv\")\n","print(test_comments[:5])\n","print(test_labels[:5])\n","print(len(test_comments))\n","print(len(test_labels))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["test part:\n"," #comment: 7000, #labels: 7000\n","['خواهشا خواهشا خواهشا واسه ارسال شیرینیها یه فکری بکنید شانس اوردم سفارش من شیرینى خشک بود فکر کنم جعبه کاملا چپ شده بود واقعا جرات نمیکنم شیرینى\\u200cتر و کیک سفارش بدم با این شرایط', 'غذا گرم رسید کیفیت و پخت گوشت عالی بود', 'قیمت سس شکلات روی بسته بندی ۱۵۰۰۰ بود ولی قیمت اعلامی در اسنپ ۱۸۰۰۰ بود. ۳۰۰۰ تومن تفاوت قیمت روی یک محصول خیلی زیاد است!', 'عکس توی پیج یه شیرینی شکری روشن رنگ هست که هممون قاعدتا بارها تستش کردیم. اما چیزی که ارسال شد یه شیرینی شکری تیره رنگ بود و کاملا متفاوت با عکس. نیاز بود عکسش هست. نمیگم بد طعم بود اما کاملا رنگش چیز دیگری هست و طعمش.', 'باز هم میگم، پیتزا نباید اینقد چرب باشه']\n","[1, 1, 0, 0, 0]\n","7000\n","7000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dF3g4Y0m6WTK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627918066333,"user_tz":-270,"elapsed":395,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"1e390221-4dea-45b7-e7ac-e8fcb12f7e23"},"source":["sa_model.sentiment_analysis_inference(test_comments[:5], device)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('خواهشا خواهشا خواهشا واسه ارسال شیرینیها یه فکری بکنید شانس اوردم سفارش من شیرینى خشک بود فکر کنم جعبه کاملا چپ شده بود واقعا جرات نمیکنم شیرینى\\u200cتر و کیک سفارش بدم با این شرایط',\n","  'SAD'),\n"," ('غذا گرم رسید کیفیت و پخت گوشت عالی بود', 'HAPPY'),\n"," ('قیمت سس شکلات روی بسته بندی ۱۵۰۰۰ بود ولی قیمت اعلامی در اسنپ ۱۸۰۰۰ بود. ۳۰۰۰ تومن تفاوت قیمت روی یک محصول خیلی زیاد است!',\n","  'SAD'),\n"," ('عکس توی پیج یه شیرینی شکری روشن رنگ هست که هممون قاعدتا بارها تستش کردیم. اما چیزی که ارسال شد یه شیرینی شکری تیره رنگ بود و کاملا متفاوت با عکس. نیاز بود عکسش هست. نمیگم بد طعم بود اما کاملا رنگش چیز دیگری هست و طعمش.',\n","  'SAD'),\n"," ('باز هم میگم، پیتزا نباید اینقد چرب باشه', 'SAD')]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"Q2WUkTiD6hBn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627918067274,"user_tz":-270,"elapsed":399,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"3045d0e6-5fd3-46b1-9bff-a703dd56cb13"},"source":["!nvidia-smi\n","!lscpu"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Mon Aug  2 15:27:46 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P0    27W /  70W |   1522MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","Architecture:        x86_64\n","CPU op-mode(s):      32-bit, 64-bit\n","Byte Order:          Little Endian\n","CPU(s):              2\n","On-line CPU(s) list: 0,1\n","Thread(s) per core:  2\n","Core(s) per socket:  1\n","Socket(s):           1\n","NUMA node(s):        1\n","Vendor ID:           GenuineIntel\n","CPU family:          6\n","Model:               79\n","Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n","Stepping:            0\n","CPU MHz:             2199.998\n","BogoMIPS:            4399.99\n","Hypervisor vendor:   KVM\n","Virtualization type: full\n","L1d cache:           32K\n","L1i cache:           32K\n","L2 cache:            256K\n","L3 cache:            56320K\n","NUMA node0 CPU(s):   0,1\n","Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C14crydH6jB6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627918392581,"user_tz":-270,"elapsed":325311,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"5e65c1b6-71e1-43c2-c033-19b7eae676ec"},"source":["evaluation_output = sa_model.evaluation(test_comments, test_labels, device, batch_size=128)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["label_count: {'SAD': 3500, 'HAPPY': 3500}\n","#samples: 7000\n","#batch: 55\n","Start to evaluate test data ...\n","inference time for step 0: 0.02268446599998697\n","inference time for step 1: 0.010085061000012274\n","inference time for step 2: 0.01097078900002657\n","inference time for step 3: 0.010206957000008288\n","inference time for step 4: 0.010765097000017931\n","inference time for step 5: 0.009803476000001865\n","inference time for step 6: 0.011400743999985252\n","inference time for step 7: 0.013068792000012763\n","inference time for step 8: 0.01011720500000024\n","inference time for step 9: 0.009900603000005503\n","inference time for step 10: 0.010073419999997668\n","inference time for step 11: 0.00994940100000008\n","inference time for step 12: 0.00999422099999947\n","inference time for step 13: 0.009699785000009342\n","inference time for step 14: 0.010640446000024895\n","inference time for step 15: 0.010101398000017525\n","inference time for step 16: 0.011719523000010668\n","inference time for step 17: 0.01029435499998499\n","inference time for step 18: 0.010768775999963509\n","inference time for step 19: 0.010215431000005992\n","inference time for step 20: 0.010641545999988011\n","inference time for step 21: 0.013782256000013149\n","inference time for step 22: 0.009692409000024327\n","inference time for step 23: 0.011236017999976866\n","inference time for step 24: 0.010174500999994507\n","inference time for step 25: 0.009525123999992502\n","inference time for step 26: 0.009752226999978575\n","inference time for step 27: 0.011108733000014581\n","inference time for step 28: 0.010374251000030199\n","inference time for step 29: 0.009667626999998902\n","inference time for step 30: 0.010121067000000039\n","inference time for step 31: 0.010841961000039646\n","inference time for step 32: 0.010285576999990553\n","inference time for step 33: 0.011110242999961883\n","inference time for step 34: 0.010024831000009726\n","inference time for step 35: 0.009837865000008605\n","inference time for step 36: 0.009779292000018813\n","inference time for step 37: 0.010213331999977981\n","inference time for step 38: 0.010276317000034396\n","inference time for step 39: 0.009951409999985117\n","inference time for step 40: 0.00991456100001642\n","inference time for step 41: 0.00985313699999324\n","inference time for step 42: 0.009900886000025366\n","inference time for step 43: 0.01226574800000435\n","inference time for step 44: 0.009696430000019518\n","inference time for step 45: 0.01473266400000739\n","inference time for step 46: 0.009860990000049696\n","inference time for step 47: 0.010433994999971219\n","inference time for step 48: 0.009947233999980654\n","inference time for step 49: 0.010177516999988256\n","inference time for step 50: 0.011084558999982619\n","inference time for step 51: 0.010103119000007155\n","inference time for step 52: 0.010402421999970102\n","inference time for step 53: 0.010747570000035012\n","inference time for step 54: 0.010433487000000241\n","average loss: 0.1821627746928822\n","total inference time: 0.5904108520001614\n","total inference time / #samples: 8.434440742859449e-05\n","Test Accuracy: 0.9348571428571428\n","Test Precision: 0.9354578970171094\n","Test Recall: 0.9348571428571428\n","Test F1-Score(weighted average): 0.9348346674669425\n","Test classification Report:\n","              precision    recall  f1-score   support\n","\n","         SAD  0.9192837466 0.9534285714 0.9360448808      3500\n","       HAPPY  0.9516320475 0.9162857143 0.9336244541      3500\n","\n","    accuracy                      0.9348571429      7000\n","   macro avg  0.9354578970 0.9348571429 0.9348346675      7000\n","weighted avg  0.9354578970 0.9348571429 0.9348346675      7000\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wx2ZIK3V6jEN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627918392582,"user_tz":-270,"elapsed":29,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}},"outputId":"d7be1d79-6b13-4104-972c-b41ea2d223d7"},"source":["for comment, true_label, predicted_label in evaluation_output[:25]:\n","  print('{}\\t{}\\t{}'.format(comment, true_label, predicted_label))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["خواهشا خواهشا خواهشا واسه ارسال شیرینیها یه فکری بکنید شانس اوردم سفارش من شیرینى خشک بود فکر کنم جعبه کاملا چپ شده بود واقعا جرات نمیکنم شیرینى‌تر و کیک سفارش بدم با این شرایط\tHAPPY\tSAD\n","غذا گرم رسید کیفیت و پخت گوشت عالی بود\tHAPPY\tHAPPY\n","قیمت سس شکلات روی بسته بندی ۱۵۰۰۰ بود ولی قیمت اعلامی در اسنپ ۱۸۰۰۰ بود. ۳۰۰۰ تومن تفاوت قیمت روی یک محصول خیلی زیاد است!\tSAD\tSAD\n","عکس توی پیج یه شیرینی شکری روشن رنگ هست که هممون قاعدتا بارها تستش کردیم. اما چیزی که ارسال شد یه شیرینی شکری تیره رنگ بود و کاملا متفاوت با عکس. نیاز بود عکسش هست. نمیگم بد طعم بود اما کاملا رنگش چیز دیگری هست و طعمش.\tSAD\tSAD\n","باز هم میگم، پیتزا نباید اینقد چرب باشه\tSAD\tSAD\n","غذا بعد از یک ساعت و بیست دقیقه به دستم رسید. دیگه این اسمش فست فود نیست خدایی. نیمه سرد. و البته کیفیتش هم خیلی پایین‌تر از دفعات قبل. هر دفعه پس میره کیفیتش. شاید دفعه آخرم باشه که خرید میکنم\tSAD\tSAD\n","فیله مرغ رژیمی خیلی خوب نپخته بود و مزه خامی می‌داد.\tSAD\tSAD\n","برنج یاده بود. خوب نیست سفارشو فقط از سر خودتون باز کنین. باید مشتری واستون ارزش داشته باشه\tSAD\tSAD\n","به غذاهای شنزار هیچ ایرادی هیچوقت نمیشه گرفت همه چی فوق العاده بود حجم عالی طعم عالی خیلی سریع رسید و پیک هم خیلی مودب بود در کل اگه بیشتر از ۵ میشد امتیاز داد با کمال میل میدادم\tHAPPY\tHAPPY\n","پیراشکی بندری نونش گرم‌ بود موادش سرد سرد بود ولی بقیه موارد خوب بودن\tHAPPY\tHAPPY\n","بنده سالاد سفارش دادم و در کنارش قارچ و سیب زمینی و نان سیر و متاسفانه اصل خوراک رو که سالاد بود رو نیاوردن\tSAD\tSAD\n","به توضیحات توجه شده بود، و مواردی که نداشتند تماس گرفتند هماهنگ کردند بابت جایگزینی\tHAPPY\tHAPPY\n","مرغ کاملا بی کیفیت و مونده بود واقعا از کترینگ پارسی انتظار نداشتم\tSAD\tSAD\n","مناسفانه یکی از اجناس گذاشته نشده بود.\tHAPPY\tHAPPY\n","آناناس نداشت برای چی؟!\tSAD\tSAD\n","عالی بود. روی نون هم پنیر زده بودن. بسیار بزرگ و خوشمزه\tHAPPY\tHAPPY\n","بی نقص و فوق العاده\tHAPPY\tHAPPY\n","طبق توضیحاتم همون طور که خواستم فرستاده بودن خیلی هم خوش مزه و عالی بود\tHAPPY\tHAPPY\n","برای سرعت ارسال سپاسگزارم\tHAPPY\tHAPPY\n","متاسفانه از کیفیت غذا و گرم نبودن غذا راضی نبودم\tSAD\tSAD\n","غذا بعد ۱ ساعت و نیم به دستمان رسید که کاملا غیر قابل قبول است\tSAD\tSAD\n","بسته بندی غذا خیلی بد بود. وقتی به دستم رسید کل آب مرغ توی کیسه ریخته شده بود. مرغ به شدت طعم موندگی میداد و کیفیتش پایین بود. سالاد مونده بود و کاهوهای سالاد کاملا قهوه‌ای شده بودن. فقط سوپش خوب بود. در کل ناراضی هستم و این آخرین سفارشم از این رستوران بود\tSAD\tSAD\n","سرد و با تاخیر دستم رسید، محتویاتش هم بسیار کم بود.\tSAD\tSAD\n","به جای دو تا شیر بدون لاکتوز، یکی پرچرب داده که به درد من نمیخوره و خواسته‌ی من این نبوده اصلا. ورداشته جای اختلاف قیمتشم یه شکلات واسه من فرستاده. من چندین بار از ایشون خرید کرده بودم و دیگه نخواهم کرد. پیک هم اومده حرف سوپری رو تکرار می‌کنه. ممنون\tSAD\tSAD\n","غدا بعد از ۲ ساعت به دستم رسید و به قدری سرد بود که واقعا ارزش خوردن نداشت. تو این مدتم ۴ بار با پشتیبانی تماس گرفتم ولی نتونستم با هیچ کدوم از همکارانتون صحبت کنم.\tSAD\tSAD\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hmuxWbMh6od7","executionInfo":{"status":"ok","timestamp":1627918393889,"user_tz":-270,"elapsed":1332,"user":{"displayName":"Zohreh Fallahnejad","photoUrl":"","userId":"17176314055126146710"}}},"source":["output_file_name = \"sentiment_analysis_snappfood_testset_{}_outputs.txt\".format(model_name.replace('/','-'))\n","with open(output_file_name, \"w\", encoding='utf8') as output_file:\n","  for comment, true_label, predicted_label in evaluation_output:\n","    output_file.write('{}\\t{}\\t{}\\n'.format(comment, true_label, predicted_label))\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","upload = drive.CreateFile({'title': output_file_name})\n","upload.SetContentFile(output_file_name)\n","upload.Upload()"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"FiqloYPk6ohX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWRZkBqt6hEk"},"source":[""],"execution_count":null,"outputs":[]}]}